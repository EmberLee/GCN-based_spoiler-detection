{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as spwords\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor as prpExecutor\n",
    "import sys\n",
    "import copy\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Data File Path\n",
    "# ============================\n",
    "# TRAIN_DATA_FILE_PATH = \"/hdd1/Spoiler_Detection/ACL/ACL_GENRE/node_edge_info_train_withid.json\"\n",
    "# VALIDATION_DATA_FILE_PATH = \"/hdd1/Spoiler_Detection/ACL/ACL_GENRE/node_edge_info_valid_withid.json\"\n",
    "# TEST_DATA_FILE_PATH = \"/hdd1/Spoiler_Detection/ACL/ACL_GENRE/node_edge_info_test_withid.json\"\n",
    "# PRE_TRAINED_WORD_EMBEDDING_FILE_PATH = \"/hdd1/Spoiler_Detection/TVTropes/glove.840B.300d.txt\"\n",
    "# GENRE_DICT = \"/hdd1/Spoiler_Detection/ACL/ACL_GENRE/genre_dict.pickle\"\n",
    "TRAIN_DATA_FILE_PATH = \"./parsed_data/node_edge_info_train_withid.json\"\n",
    "VALIDATION_DATA_FILE_PATH = \"./parsed_data/node_edge_info_valid_withid.json\"\n",
    "TEST_DATA_FILE_PATH = \"./parsed_data/node_edge_info_test_withid.json\"\n",
    "PRE_TRAINED_WORD_EMBEDDING_FILE_PATH = \"./parsed_data/glove.840B.300d.txt\"\n",
    "GENRE_DICT = \"./parsed_data/genre_dict.pickle\"\n",
    "\n",
    "# ============================\n",
    "# Model Hyper Parameter\n",
    "# ============================\n",
    "EMBEDDING_DIM = 300\n",
    "GENRE_EMBEDDING_DIM = 50\n",
    "HIDDEN_STATES = [100, 100]\n",
    "NUM_FILTERS = 50\n",
    "FILTER_SIZES = [2,3]\n",
    "NUM_HEADS = 3\n",
    "LEAKY_ALPHA = 0.2\n",
    "\n",
    "# ============================\n",
    "# Training Hyper Parameter\n",
    "# ============================\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.5\n",
    "RANDOM_SEED = 26\n",
    "\n",
    "# ============================\n",
    "# Set Random Seed\n",
    "# ============================\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data...\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Data Pre-Processing\n",
    "# ============================\n",
    "def load_data(train_file_path, validation_file_path, test_file_path, genre_dict_file):\n",
    "    with open(train_file_path) as f:\n",
    "        train = json.load(f)\n",
    "    with open(validation_file_path) as f:\n",
    "        validation = json.load(f)\n",
    "    with open(test_file_path) as f:\n",
    "        test = json.load(f)\n",
    "    with open(genre_dict_file, \"rb\") as f:\n",
    "        genre_dict = pickle.load(f)\n",
    "        \n",
    "    return train, validation, test, genre_dict\n",
    "\n",
    "# ============================\n",
    "# Data Pre Processing\n",
    "# ============================\n",
    "print(\"Load Data...\")\n",
    "start = time.time()\n",
    "train, validation, test, genre_dict = load_data(TRAIN_DATA_FILE_PATH,\n",
    "                                                VALIDATION_DATA_FILE_PATH, \n",
    "                                                TEST_DATA_FILE_PATH,\n",
    "                                                GENRE_DICT)\n",
    "print(int(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Dictionary...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8dae2a95ed4fef8a7fc4a3709eb888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1091952), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f4a575bd804944a51f98a8619c76f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b87a75e2a75496697dcba3b7ea3621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=276081), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d38a63a1194820bf44961423b06eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12911731), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: 10230700, valid: 93500, test: 2587531, overall: 12911731, max_len: 50\n"
     ]
    }
   ],
   "source": [
    "def check(review):\n",
    "    for line in review:\n",
    "        a = len(line.split(\"\\t\")[0].split())\n",
    "        if a>50:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def make_dictionary(_train, _validation, _test, genre_dict):\n",
    "    train, validation, test = [], [], []\n",
    "    for line in tqdm_notebook(_train):\n",
    "        if check(line[\"text_info\"]):\n",
    "            train += [_line for _line in line[\"text_info\"]]\n",
    "    for line in tqdm_notebook(_validation):\n",
    "        if check(line[\"text_info\"]):\n",
    "            validation += [_line for _line in line[\"text_info\"]]\n",
    "    for line in tqdm_notebook(_test):\n",
    "        if check(line[\"text_info\"]):\n",
    "            test += [_line for _line in line[\"text_info\"]]\n",
    "        \n",
    "    data = []\n",
    "    data += train\n",
    "    data += validation\n",
    "    data += test\n",
    "    \n",
    "    global maximum_length\n",
    "    maximum_length = max([len(line.split(\"\\t\")[0].split()) for line in data])\n",
    "    global maximum_genre_length\n",
    "    maximum_genre_length = max([len(value) for _, value in genre_dict.items()])\n",
    "    \n",
    "    word2id = {\"<PAD>\":0}\n",
    "    id2word = [\"<PAD>\"]\n",
    "    edge2id = {\"<NONE>\":0, \"<SELF>\": 1}\n",
    "    id2edge = [\"<NONE>\", \"<SELF>\"]\n",
    "    genre2id = {\"<PAD>\":0}\n",
    "    id2genre = [\"<PAD>\"]\n",
    "    \n",
    "    for line in tqdm_notebook(data):\n",
    "        tokens = line.split(\"\\t\")\n",
    "        for word in tokens[0].split():\n",
    "            if word not in word2id:\n",
    "                word2id[word] = len(word2id)\n",
    "                id2word.append(word)\n",
    "        for edges in tokens[3:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            if edge not in edge2id:\n",
    "                edge2id[edge] = len(edge2id)\n",
    "                id2edge.append(edge)\n",
    "            del _tokens\n",
    "        del tokens\n",
    "    del data, _train, _validation, _test\n",
    "    \n",
    "    book2genre = {}\n",
    "    for key, value in genre_dict.items():\n",
    "        for genre in value:\n",
    "            if genre not in genre2id:\n",
    "                genre2id[genre] = len(genre2id)\n",
    "                id2genre.append(genre)\n",
    "        book2genre[key] = [genre2id[genre] for genre in value]\n",
    "\n",
    "    num_edges = len(edge2id)\n",
    "    for i in range(num_edges):\n",
    "        key = id2edge[i]\n",
    "        if key != \"<NONE>\" and key != \"<SELF>\":\n",
    "            opposite = key+\"'\"\n",
    "            edge2id[opposite] = edge2id[key]+num_edges-2\n",
    "    \n",
    "    return train, validation, test, word2id, id2word, edge2id, id2edge, genre2id, id2genre, book2genre, maximum_length, maximum_genre_length\n",
    "\n",
    "print(\"Make Dictionary...\")\n",
    "start = time.time()\n",
    "_train, _validation, _test, word2id, id2word, edge2id, id2edge, genre2id, id2genre, book2genre, maximum_length, maximum_genre_length \\\n",
    "= make_dictionary(train, validation, test, genre_dict)\n",
    "print('train: {}, valid: {}, test: {}, overall: {}, max_len: {}'.format(len(_train), len(_validation), len(_test), len(_train) + len(_validation) + len(_test), maximum_length))\n",
    "\n",
    "book2genre[24711433] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Input as Index...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a52b9d60e948c3859e6bffc74c3c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10230700), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63864c9dd2b84e0b84b20924027ad9c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=93500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e28362c934f944fda108b2673c49848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2587531), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "406\n"
     ]
    }
   ],
   "source": [
    "def make_input_data_as_index(_data, word2id, edge2id):\n",
    "    data = []\n",
    "    for line in tqdm_notebook(_data):\n",
    "        tokens = line.split(\"\\t\")\n",
    "        sentence, label, book = tokens[0], int(tokens[1]), tokens[2]\n",
    "        _edges = []\n",
    "        for edges in tokens[3:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            _edges.append(\":\".join([start, end, str(edge2id[edge])]))\n",
    "            del _tokens\n",
    "        data.append([sentence, label, \" \".join(_edges), book])\n",
    "        del tokens\n",
    "    return data\n",
    "\n",
    "print(\"Make Input as Index...\")\n",
    "start = time.time()\n",
    "_train = make_input_data_as_index(_train, word2id, edge2id)\n",
    "_validation = make_input_data_as_index(_validation, word2id, edge2id)\n",
    "_test = make_input_data_as_index(_test, word2id, edge2id)\n",
    "print(int(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make Adjacency Matrix...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419bbbe70fb44ec69dfd8e29f5e2be93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10230700), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed433ed03cd3412eb9827db466e9556c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=93500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d632b22a10fa4a78b5636a5709c669b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2587531), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2568\n"
     ]
    }
   ],
   "source": [
    "def make_input_adjacency_matrix(line):\n",
    "    sentence, label, edges, book = line[0], float(line[1]), line[2].split(), line[3]\n",
    "    edges = np.asarray([edge.split(\":\") for edge in edges])\n",
    "    adjacency_matrix = matrix_to_torch_sparse_tensor(edges, maximum_length)\n",
    "    \n",
    "    return [sentence, adjacency_matrix, label, book]\n",
    "\n",
    "def matrix_to_torch_sparse_tensor(edges, maximum_length):\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((edges[:, 0], edges[:, 1])).astype(np.int64))\n",
    "    values = torch.from_numpy(edges[:, 2].astype(np.int64))\n",
    "    shape = torch.Size((maximum_length, maximum_length))\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "print(\"Make Adjacency Matrix...\")\n",
    "start = time.time()\n",
    "pool = prpExecutor(max_workers=5)\n",
    "# _train = list(pool.map(make_input_adjacency_matrix, tqdm_notebook(_train)))\n",
    "# _validation = list(pool.map(make_input_adjacency_matrix, tqdm(_validation)))\n",
    "_train = [make_input_adjacency_matrix(line) for line in tqdm_notebook(_train)]\n",
    "_validation = [make_input_adjacency_matrix(line) for line in tqdm_notebook(_validation)]\n",
    "_test = [make_input_adjacency_matrix(line) for line in tqdm_notebook(_test)]\n",
    "del pool\n",
    "print(int(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Pre-trained Word Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:2: ResourceWarning: unclosed file <_io.TextIOWrapper name='/hdd1/Spoiler_Detection/TVTropes/glove.840B.300d.txt' mode='r' encoding='UTF-8'>\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc33207e76941c2b51051bba9a94a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2196017), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7986dd37bf14aa9be1c312d46e09421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=622950), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_pre_trained_word_embedding(word_embedding_file_path, word2id):\n",
    "    lines = [line.strip() for line in open(word_embedding_file_path).readlines()]\n",
    "    pre_trained_word_embedding = {}\n",
    "    for line in tqdm_notebook(lines):\n",
    "        tokens = line.split()\n",
    "        if len(tokens) != 301:\n",
    "            continue\n",
    "        pre_trained_word_embedding[tokens[0]] = np.asarray(tokens[1:]).astype(np.float32)\n",
    "        \n",
    "    word_embedding = np.random.uniform(size=(len(word2id), EMBEDDING_DIM))\n",
    "    for key in tqdm_notebook(word2id.keys()):\n",
    "        if key in pre_trained_word_embedding:\n",
    "            word_embedding[word2id[key]] = pre_trained_word_embedding[key]\n",
    "    \n",
    "    word_embedding[0] = np.zeros(EMBEDDING_DIM)\n",
    "    return torch.from_numpy(word_embedding)\n",
    "\n",
    "print(\"Load Pre-trained Word Embedding...\")\n",
    "word_embedding = load_pre_trained_word_embedding(PRE_TRAINED_WORD_EMBEDDING_FILE_PATH, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size, word2id, book2genre, is_train=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if is_train:\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if len(data) % batch_size == 0:\n",
    "        batch_num = int(len(data)/batch_size)\n",
    "    else:\n",
    "        batch_num = int(len(data)/batch_size) + 1\n",
    "        \n",
    "    for i in range(batch_num):\n",
    "        left = i*batch_size\n",
    "        right = min((i+1)*batch_size, len(data))\n",
    "        \n",
    "        sentences = []\n",
    "        adjacency_matrics = []\n",
    "        labels = []\n",
    "        genres = []\n",
    "        \n",
    "        for j in indices[left:right]:\n",
    "            sentence = [word2id[word] for word in data[j][0].split()]\n",
    "            sentence += [0]*(maximum_length - len(sentence))\n",
    "            sentences.append(sentence)\n",
    "            adjacency_matrics.append(data[j][1])\n",
    "            labels.append(data[j][2])\n",
    "            _genres = book2genre[int(data[j][3])]\n",
    "            _genres += [0]*(maximum_genre_length - len(_genres))\n",
    "            genres.append(_genres)\n",
    "        \n",
    "        yield sentences, adjacency_matrics, labels, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Model\n",
    "# ============================\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_dim, self.output_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(self.output_dim))\n",
    "\n",
    "    def forward(self, x, attention_weight):\n",
    "        x = x*attention_weight.unsqueeze(3)\n",
    "        x = x.sum(2)\n",
    "        output = torch.matmul(x, self.weight)\n",
    "        output = output + self.bias\n",
    "\n",
    "        return output\n",
    "    \n",
    "class GenreEncoder(nn.Module):\n",
    "    def __init__(self, num_filters, filter_sizes, genre_embedding_dim, maximum_genre_length):\n",
    "        super(GenreEncoder, self).__init__()\n",
    "        \n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.genre_embedding_dim = genre_embedding_dim\n",
    "        self.maximum_genre_length = maximum_genre_length\n",
    "        \n",
    "        # ==============================\n",
    "        # 1D CNN\n",
    "        # ==============================\n",
    "        self.cnn = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(self.genre_embedding_dim, self.num_filters, size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.maximum_genre_length - size + 1)\n",
    "        ) for size in self.filter_sizes])\n",
    "\n",
    "\n",
    "    def forward(self, genres):\n",
    "        genres = genres.transpose(1,2)\n",
    "        convs = [conv(genres).squeeze() for conv in self.cnn]\n",
    "\n",
    "        return torch.cat(convs, dim=1)\n",
    "        \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, alpha, input_dim, output_dim, num_edges, maximum_length):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.maximum_length = maximum_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_edges = num_edges\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # =============================================\n",
    "        # Data Preparation\n",
    "        # =============================================\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_dim, self.output_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "        \n",
    "        self.edge_embedding \\\n",
    "        = nn.Embedding(self.num_edges, self.output_dim, padding_idx = 0)\n",
    "        nn.init.xavier_normal_(self.edge_embedding.weight)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, x, adjacency_matrics):\n",
    "        # ==================================\n",
    "        # x: (B, N, H)\n",
    "        # adjacency_matrics: (B, N, N)\n",
    "        hidden = torch.matmul(x, self.weight) # (B, N, H')\n",
    "        hidden = hidden.unsqueeze(1) # (B, 1, N, H')\n",
    "        hidden = hidden.expand(hidden.size(0),\n",
    "                               self.maximum_length,\n",
    "                               self.maximum_length,\n",
    "                               self.output_dim) # (B, N, N, H')\n",
    "            \n",
    "        edges = self.edge_embedding(adjacency_matrics)\n",
    "\n",
    "        attention_weight = hidden*edges # (B, N, N, H')\n",
    "        attention_weight = torch.sum(attention_weight, dim=3) # (B, N, N)\n",
    "        attention_weight = self.leakyrelu(attention_weight)\n",
    "        \n",
    "        zero_vec = -9e15*torch.ones_like(attention_weight)\n",
    "        attention_weight = torch.where(adjacency_matrics > 0, attention_weight, zero_vec)\n",
    "        attention_weight = torch.softmax(attention_weight, dim=2)\n",
    "        \n",
    "        return attention_weight\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_words, \n",
    "                 num_edges, \n",
    "                 num_genres,\n",
    "                 alpha, \n",
    "                 embedding_dim, \n",
    "                 genre_embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 num_filters,\n",
    "                 filter_sizes,\n",
    "                 maximum_length,\n",
    "                 maximum_genre_length,\n",
    "                 pre_trained, \n",
    "                 dropout_rate):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.num_edges = num_edges\n",
    "        self.num_genres = num_genres\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.genre_embedding_dim = genre_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.maximum_length = maximum_length\n",
    "        self.maximum_genre_length = maximum_genre_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # =============================================\n",
    "        # Data Preparation\n",
    "        # =============================================\n",
    "        self.word_embedding \\\n",
    "        = nn.Embedding.from_pretrained(pre_trained, freeze=False)\n",
    "        self.genre_embedding \\\n",
    "        = nn.Embedding(self.num_genres, self.genre_embedding_dim, padding_idx = 0)\n",
    "        \n",
    "        self.attention_1 = Attention(self.alpha,\n",
    "                                     2*self.hidden_dim[0], \n",
    "                                     self.hidden_dim[0], \n",
    "                                     self.num_edges, \n",
    "                                     self.maximum_length)\n",
    "\n",
    "        self.attention_2 = Attention(self.alpha,\n",
    "                                     self.hidden_dim[0], \n",
    "                                     self.hidden_dim[1], \n",
    "                                     self.num_edges, \n",
    "                                     self.maximum_length)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim[0], bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.gcn_layer_1 \\\n",
    "        = GCNLayer(2*self.hidden_dim[0], self.hidden_dim[0])\n",
    "        self.gcn_layer_2 \\\n",
    "        = GCNLayer(self.hidden_dim[0], self.hidden_dim[1])\n",
    "        \n",
    "        self.genre_encoder = GenreEncoder(self.num_filters, \n",
    "                                          self.filter_sizes, \n",
    "                                          self.genre_embedding_dim, \n",
    "                                          self.maximum_genre_length)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.hidden_dim[1], self.hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.hidden_dim[1], 2)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, sentences, adjacency_matrics, genres):\n",
    "        zero_vec = torch.zeros_like(adjacency_matrics)\n",
    "        adjacency_matrics_t = torch.where(adjacency_matrics > 0, \n",
    "                                          adjacency_matrics + int((self.num_edges-2)/2), \n",
    "                                          zero_vec)\n",
    "        adjacency_matrics_t = adjacency_matrics_t.transpose(1,2)\n",
    "        eye = torch.eye(adjacency_matrics.size(1), dtype=torch.long).cuda()\n",
    "        eye = eye.unsqueeze(0).expand(sentences.size(0),\n",
    "                                      self.maximum_length,\n",
    "                                      self.maximum_length)\n",
    "        adjacency_matrics = adjacency_matrics \\\n",
    "                          + adjacency_matrics_t \\\n",
    "                          + eye # (B, N, N)\n",
    "        \n",
    "        embedded_words = self.word_embedding(sentences) # (B, N, D)\n",
    "        h0 = torch.zeros(2, sentences.size(0), self.hidden_dim[0]).cuda() # 2 for bidirection \n",
    "        c0 = torch.zeros(2, sentences.size(0), self.hidden_dim[0]).cuda()\n",
    "        self.lstm.flatten_parameters()\n",
    "        lstm = self.lstm(embedded_words, (h0, c0))[0] # (B, N, 2H)\n",
    "        attention_weight_1 = self.attention_1(lstm, adjacency_matrics)\n",
    "        lstm = lstm.unsqueeze(1)\n",
    "        lstm = lstm.expand(lstm.size(0),\n",
    "                          self.maximum_length,\n",
    "                          self.maximum_length,\n",
    "                          2*self.hidden_dim[0])\n",
    "        \n",
    "        gcn_1 = self.gcn_layer_1(lstm, attention_weight_1)\n",
    "        gcn_1 = torch.relu(gcn_1) # B X N X H\n",
    "        gcn_1 = self.dropout(gcn_1)\n",
    "        \n",
    "        attention_weight_2 = self.attention_2(gcn_1, adjacency_matrics)\n",
    "        gcn_1 = gcn_1.unsqueeze(1)\n",
    "        gcn_1 = gcn_1.expand(gcn_1.size(0),\n",
    "                          self.maximum_length,\n",
    "                          self.maximum_length,\n",
    "                          self.hidden_dim[0])\n",
    "        gcn_2 = self.gcn_layer_2(gcn_1, attention_weight_2)\n",
    "        gcn_2 = torch.relu(gcn_2) # (B, N, H')\n",
    "        \n",
    "        genres = self.genre_embedding(genres) # (B, N', G)\n",
    "        genre_features = self.genre_encoder(genres) # (B, H')\n",
    "        attention_weight_3 = (gcn_2*genre_features.unsqueeze(1)).sum(2) # (B, N)\n",
    "        zero_vec = -9e15*torch.ones_like(attention_weight_3)\n",
    "        attention_weight_3 = torch.where(sentences > 0, attention_weight_3, zero_vec)\n",
    "        attention_weight_3 = torch.softmax(attention_weight_3, dim=1) # (B, N)\n",
    "        \n",
    "        sentence_representations = (gcn_2*attention_weight_3.unsqueeze(2)).sum(1) # (B, H')\n",
    "        \n",
    "        output = self.output_layer(sentence_representations)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initializing..\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Model Initialize\n",
    "# =============================================\n",
    "print(\"Model Initializing..\")\n",
    "weight = torch.ones(2).cuda()\n",
    "weight[0] = 0.05\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "model = Model(len(word2id), len(edge2id), len(genre2id), \n",
    "              LEAKY_ALPHA, EMBEDDING_DIM, GENRE_EMBEDDING_DIM, HIDDEN_STATES, \n",
    "              NUM_FILTERS, FILTER_SIZES, maximum_length, maximum_genre_length, \n",
    "              word_embedding.type(\"torch.FloatTensor\"), DROPOUT_RATE)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training [A/P/R/F]: [2.5829/0.0258/1.0000/0.0504]\n",
      "Model Training..\n",
      "Epoch: [1/300] Eval Batch: [183/183] [A/P/R/F]: [87.5316/0.1025/0.4932/0.1697] Loss: 10379.9422\n",
      " Current Best:)\n",
      "Epoch: [2/300] Eval Batch: [183/183] [A/P/R/F]: [86.0086/0.0936/0.5089/0.1582] Loss: 9891.2980\n",
      "Epoch: [3/300] Eval Batch: [183/183] [A/P/R/F]: [86.0396/0.0962/0.5246/0.1626] Loss: 9703.9916\n",
      "Epoch: [4/300] Eval Batch: [183/183] [A/P/R/F]: [83.5027/0.0876/0.5718/0.1519] Loss: 9589.6648\n",
      "Epoch: [5/300] Eval Batch: [183/183] [A/P/R/F]: [85.5112/0.0947/0.5387/0.1611] Loss: 9513.9335\n",
      "Epoch: [6/300] Eval Batch: [183/183] [A/P/R/F]: [85.5743/0.0983/0.5611/0.1673] Loss: 9458.6429\n",
      "Epoch: [7/300] Eval Batch: [183/183] [A/P/R/F]: [83.6460/0.0881/0.5702/0.1526] Loss: 9413.1096\n",
      "Epoch: [8/300] Eval Batch: [183/183] [A/P/R/F]: [90.0695/0.1213/0.4555/0.1916] Loss: 9381.7152\n",
      " Current Best:)\n",
      "Epoch: [9/300] Batch: [7828/19982]"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Model Training\n",
    "# =============================================\n",
    "model.eval()\n",
    "\n",
    "batches = make_batch(_validation, BATCH_SIZE, word2id, book2genre, False)\n",
    "if len(_validation) % BATCH_SIZE == 0:\n",
    "    batch_num = int(len(_validation)/BATCH_SIZE)\n",
    "else:\n",
    "    batch_num = int(len(_validation)/BATCH_SIZE) + 1\n",
    "\n",
    "step = 0\n",
    "count = 0\n",
    "correct = 0\n",
    "positive_answer = 0\n",
    "positive_actual = 0\n",
    "for batch in batches:\n",
    "        sentences, adjacency_matrics, labels, genres = batch\n",
    "        input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "        input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "        input_labels = torch.tensor(labels, dtype=torch.long).cuda()\n",
    "        input_genres = torch.tensor(genres, dtype=torch.long).cuda()\n",
    "        logits = torch.argmax(model(input_sentences, input_adjacency_matrics, input_genres), dim=1)\n",
    "\n",
    "        positive_answer += logits.sum().item()\n",
    "        positive_actual += (input_labels == 1.0).float().sum().item()\n",
    "        correct+=(logits*input_labels).sum().item()\n",
    "        count+= (logits==input_labels).sum().item()\n",
    "        step+=1\n",
    "                \n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r\" + \"Batch: [{}/{}]\".format(step, batch_num))\n",
    "\n",
    "accuracy = 100*float(count)/len(_validation)\n",
    "if positive_answer == 0:\n",
    "    precision = 0.0\n",
    "else:\n",
    "    precision = float(correct)/positive_answer\n",
    "recall = float(correct)/positive_actual\n",
    "if (precision+recall) == 0.0:\n",
    "    f1 = 0.0\n",
    "else:\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\r\" + \"Before Training [A/P/R/F]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}]\".format(accuracy, precision, recall, f1))\n",
    "\n",
    "best_model = {}\n",
    "best = np.zeros(3)\n",
    "print(\"\\nModel Training..\")\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    if len(_train) % BATCH_SIZE == 0:\n",
    "        batch_num = int(len(_train)/BATCH_SIZE)\n",
    "    else:\n",
    "        batch_num = int(len(_train)/BATCH_SIZE) + 1\n",
    "    \n",
    "    loss = .0\n",
    "    batches = make_batch(_train, BATCH_SIZE, word2id, book2genre)\n",
    "    step = 0\n",
    "\n",
    "    for batch in batches:\n",
    "        sentences, adjacency_matrics, labels, genres = batch\n",
    "        input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "        input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "        input_labels = torch.tensor(labels, dtype=torch.long).cuda()\n",
    "        input_genres = torch.tensor(genres, dtype=torch.long).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_sentences, input_adjacency_matrics, input_genres)\n",
    "        _loss = criterion(logits, input_labels).sum()\n",
    "        _loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += _loss.item()\n",
    "        step+=1\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r\" + \"Epoch: [{}/{}] Batch: [{}/{}]\".format(i+1, EPOCHS, step, batch_num))\n",
    "    \n",
    "    if (i+1) % 1 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        batches = make_batch(_validation, BATCH_SIZE, word2id, book2genre, False)\n",
    "        if len(_validation) % BATCH_SIZE == 0:\n",
    "            batch_num = int(len(_validation)/BATCH_SIZE)\n",
    "        else:\n",
    "            batch_num = int(len(_validation)/BATCH_SIZE) + 1\n",
    "\n",
    "        step = 0\n",
    "        count = 0\n",
    "        correct = 0\n",
    "        positive_answer = 0\n",
    "        positive_actual = 0\n",
    "        for batch in batches:\n",
    "            sentences, adjacency_matrics, labels, genres = batch\n",
    "            input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "            input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "            input_labels = torch.tensor(labels, dtype=torch.long).cuda()\n",
    "            input_genres = torch.tensor(genres, dtype=torch.long).cuda()\n",
    "            logits = torch.argmax(model(input_sentences, input_adjacency_matrics, input_genres), dim=1)\n",
    "\n",
    "            positive_answer += logits.sum().item()\n",
    "            positive_actual += (input_labels == 1.0).float().sum().item()\n",
    "            correct+=(logits*input_labels).sum().item()\n",
    "            count+= (logits==input_labels).sum().item()\n",
    "            step+=1\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"\\r\" + \"Epoch: [{}/{}] Eval Batch: [{}/{}]\".format(i+1, EPOCHS, step, batch_num))\n",
    "\n",
    "        accuracy = 100*float(count)/len(_validation)\n",
    "        if positive_answer == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = float(correct)/positive_answer\n",
    "        recall = float(correct)/positive_actual\n",
    "        if (precision+recall) == 0.0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "        print(\" [A/P/R/F]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}] Loss: {:.4f}\".format(accuracy, precision, recall, f1, loss))\n",
    "        if f1 > best[2]:\n",
    "            best = precision, recall, f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print(\" Current Best:)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
