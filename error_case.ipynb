{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os; import sys; import logging\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as spwords\n",
    "import time\n",
    "from datetime import datetime\n",
    "import random\n",
    "import copy; import pdb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s <%(levelname)s> %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Data File Path\n",
    "# ============================\n",
    "\n",
    "# New Path(server) (20.08.11)\n",
    "TRAIN_DATA_FILE_PATH = \"./parsed_data/node_edge_info_train_withid.json\"\n",
    "VALIDATION_DATA_FILE_PATH = \"./parsed_data/node_edge_info_valid_withid.json\"\n",
    "TEST_DATA_FILE_PATH = \"./parsed_data/node_edge_info_test_withid.json\"\n",
    "PRE_TRAINED_WORD_EMBEDDING_FILE_PATH = \"./parsed_data/glove.840B.300d.txt\"\n",
    "GENRE_DICT = \"./parsed_data/genre_dict.pickle\"\n",
    "tv_train_path = './tvtropes_data/train_parsed.pickle'\n",
    "tv_test_path = './tvtropes_data/test_parsed.pickle'\n",
    "tv_train = pickle.load(open(tv_train_path, 'rb'))\n",
    "tv_test = pickle.load(open(tv_test_path, 'rb'))\n",
    "\n",
    "# PRE_TRAINED_WORD_EMBEDDING_FILE_PATH = \"./parsed_data/glove.6B.300d.txt\"\n",
    "\n",
    "# ============================\n",
    "# Model Hyper Parameter\n",
    "# ============================\n",
    "EMBEDDING_DIM = 300\n",
    "GENRE_EMBEDDING_DIM = 50\n",
    "HIDDEN_STATES = [100, 100]\n",
    "NUM_FILTERS = 50\n",
    "FILTER_SIZES = [2,3]\n",
    "NUM_HEADS = 3\n",
    "LEAKY_ALPHA = 0.2\n",
    "\n",
    "# ============================\n",
    "# Training Hyper Parameter\n",
    "# ============================\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 1024\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.5\n",
    "RANDOM_SEED = 26\n",
    "\n",
    "# ============================\n",
    "# Set Random Seed\n",
    "# ============================\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 10:40:18,137 - root <INFO> Load Data...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Data Pre-Processing\n",
    "# ============================\n",
    "def load_data(train_file_path, validation_file_path, test_file_path, genre_dict_file):\n",
    "    with open(train_file_path) as f:\n",
    "        train = json.load(f)\n",
    "    with open(validation_file_path) as f:\n",
    "        validation = json.load(f)\n",
    "    with open(test_file_path) as f:\n",
    "        test = json.load(f)\n",
    "    with open(genre_dict_file, \"rb\") as f:\n",
    "        genre_dict = pickle.load(f)\n",
    "\n",
    "    return train, validation, test, genre_dict\n",
    "\n",
    "# ============================\n",
    "# Data Pre Processing\n",
    "# ============================\n",
    "logger.info(\"Load Data...\")\n",
    "start = time.time()\n",
    "train, validation, test, genre_dict = load_data(TRAIN_DATA_FILE_PATH,\n",
    "                                                VALIDATION_DATA_FILE_PATH,\n",
    "                                                TEST_DATA_FILE_PATH,\n",
    "                                                GENRE_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 10:41:12,363 - root <INFO> Make Dictionary...\n",
      "100%|██████████| 1091952/1091952 [00:25<00:00, 43101.02it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 42997.82it/s]\n",
      "100%|██████████| 276081/276081 [00:06<00:00, 44009.41it/s]\n",
      "100%|██████████| 12911731/12911731 [02:43<00:00, 79039.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count, b: 0, 219536978\n",
      "train: 10230700, valid: 93500, test: 2587531, overall: 12911731, max_len: 50\n"
     ]
    }
   ],
   "source": [
    "def check(review):\n",
    "    for line in review:\n",
    "        a = len(line.split(\"\\t\")[0].split())\n",
    "        if a>50:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def make_dictionary(_train, _validation, _test, genre_dict):\n",
    "    train, validation, test = [], [], []\n",
    "    for line in tqdm(_train):\n",
    "        if check(line[\"text_info\"]):\n",
    "            train += line[\"text_info\"]\n",
    "    for line in tqdm(_validation):\n",
    "        if check(line[\"text_info\"]):\n",
    "            validation += line[\"text_info\"]\n",
    "    for line in tqdm(_test):\n",
    "        if check(line[\"text_info\"]):\n",
    "            test += line[\"text_info\"]\n",
    "\n",
    "    data = []\n",
    "    data += train\n",
    "    data += validation\n",
    "    data += test\n",
    "\n",
    "    global maximum_length\n",
    "    # maximum_length = 50\n",
    "    maximum_length = max([len(line.split(\"\\t\")[0].split()) for line in data])\n",
    "    global maximum_genre_length\n",
    "    maximum_genre_length = max([len(value) for _, value in genre_dict.items()])\n",
    "\n",
    "    word2id = {\"<PAD>\":0}\n",
    "    id2word = [\"<PAD>\"]\n",
    "    edge2id = {\"<NONE>\":0, \"<SELF>\": 1}\n",
    "    id2edge = [\"<NONE>\", \"<SELF>\"]\n",
    "    genre2id = {\"<PAD>\":0}\n",
    "    id2genre = [\"<PAD>\"]\n",
    "\n",
    "    count = 0\n",
    "    b = 0\n",
    "\n",
    "    for line in tqdm(data):\n",
    "        tokens = line.split(\"\\t\")\n",
    "        b += len(tokens[0].split())\n",
    "        if len(tokens[0].split()) > 50:\n",
    "            count+=1\n",
    "        for word in tokens[0].split():\n",
    "            if word not in word2id:\n",
    "                word2id[word] = len(word2id)\n",
    "                id2word.append(word)\n",
    "        for edges in tokens[3:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            if edge not in edge2id:\n",
    "                edge2id[edge] = len(edge2id)\n",
    "                id2edge.append(edge)\n",
    "            del _tokens\n",
    "        del tokens\n",
    "    del data, _train, _validation, _test\n",
    "\n",
    "    book2genre = {}\n",
    "    for key, value in genre_dict.items():\n",
    "        for genre in value:\n",
    "            if genre not in genre2id:\n",
    "                genre2id[genre] = len(genre2id)\n",
    "                id2genre.append(genre)\n",
    "        book2genre[key] = [genre2id[genre] for genre in value]\n",
    "\n",
    "    num_edges = len(edge2id)\n",
    "    for i in range(num_edges):\n",
    "        key = id2edge[i]\n",
    "        if key != \"<NONE>\" and key != \"<SELF>\":\n",
    "            opposite = key+\"'\"\n",
    "            edge2id[opposite] = edge2id[key]+num_edges-2\n",
    "\n",
    "    print('count, b: {}, {}'.format(count, b))\n",
    "    return train, validation, test, word2id, id2word, edge2id, id2edge, genre2id, id2genre, book2genre, maximum_length, maximum_genre_length\n",
    "\n",
    "logger.info(\"Make Dictionary...\")\n",
    "_train, _validation, _test, word2id, id2word, edge2id, id2edge, genre2id, id2genre, book2genre, maximum_length, maximum_genre_length = make_dictionary(train, validation, test, genre_dict)\n",
    "book2genre[24711433] = []\n",
    "\n",
    "print('train: {}, valid: {}, test: {}, overall: {}, max_len: {}'.format(len(_train), len(_validation), len(_test), len(_train) + len(_validation) + len(_test), maximum_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 10:44:53,900 - root <INFO> Make Input as Index...\n",
      "100%|██████████| 10230700/10230700 [02:54<00:00, 58731.59it/s]\n",
      "100%|██████████| 93500/93500 [00:01<00:00, 64417.66it/s]\n",
      "100%|██████████| 2587531/2587531 [00:44<00:00, 58605.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def make_input_data_as_index(_data, word2id, edge2id):\n",
    "    data = []\n",
    "    for line in tqdm(_data):\n",
    "        tokens = line.split(\"\\t\")\n",
    "        sentence, label, book = tokens[0], int(tokens[1]), tokens[2]\n",
    "        _edges = []\n",
    "        for edges in tokens[3:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            _edges.append(\":\".join([start, end, str(edge2id[edge])]))\n",
    "            del _tokens\n",
    "        data.append([sentence, label, \" \".join(_edges), book])\n",
    "        del tokens\n",
    "    return data\n",
    "\n",
    "logger.info(\"Make Input as Index...\")\n",
    "start = time.time()\n",
    "_train = make_input_data_as_index(_train, word2id, edge2id)\n",
    "_validation = make_input_data_as_index(_validation, word2id, edge2id)\n",
    "_test = make_input_data_as_index(_test, word2id, edge2id)\n",
    "# ['<ROOT> This series is so much fun !', 0, '0:6:2 2:1:6 5:4:13 6:2:5 6:7:4 6:5:10 6:3:9', '9595620']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 10:48:33,880 - root <INFO> Make Adjacency Matrix...\n",
      "100%|██████████| 10230700/10230700 [19:11<00:00, 8882.77it/s] \n",
      "100%|██████████| 93500/93500 [00:10<00:00, 9137.02it/s]\n",
      "100%|██████████| 2587531/2587531 [04:57<00:00, 8711.89it/s] \n"
     ]
    }
   ],
   "source": [
    "def make_input_adjacency_matrix(line):\n",
    "    sentence, label, edges, book = line[0], float(line[1]), line[2].split(), line[3]\n",
    "    edges = np.asarray([edge.split(\":\") for edge in edges])\n",
    "    adjacency_matrix = matrix_to_torch_sparse_tensor(edges, maximum_length)\n",
    "\n",
    "    return [sentence, adjacency_matrix, label, book]\n",
    "\n",
    "def matrix_to_torch_sparse_tensor(edges, maximum_length):\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((edges[:, 0], edges[:, 1])).astype(np.int64))\n",
    "    values = torch.from_numpy(edges[:, 2].astype(np.int64))\n",
    "    shape = torch.Size((maximum_length, maximum_length))\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "logger.info(\"Make Adjacency Matrix...\")\n",
    "train_path = './adj_mats/train_adj.pickle'\n",
    "val_path = './adj_mats/val_adj.pickle'\n",
    "test_path = './adj_mats/test_adj.pickle'\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    _train = [make_input_adjacency_matrix(line) for line in tqdm(_train)]\n",
    "    _validation = [make_input_adjacency_matrix(line) for line in tqdm(_validation)]\n",
    "    _test = [make_input_adjacency_matrix(line) for line in tqdm(_test)]\n",
    "else:\n",
    "    _train = pickle.load(open(train_path, 'rb'))\n",
    "    _validation = pickle.load(open(val_path, 'rb'))\n",
    "    _test = pickle.load(open(test_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 11:12:55,259 - root <INFO> Load Pre-trained Word Embedding...\n",
      "100%|██████████| 2196017/2196017 [06:36<00:00, 5545.26it/s]\n",
      "100%|██████████| 622950/622950 [00:00<00:00, 798108.40it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_pre_trained_word_embedding(word_embedding_file_path, word2id):\n",
    "    lines = [line.strip() for line in open(word_embedding_file_path).readlines()]\n",
    "    pre_trained_word_embedding = {}\n",
    "    for line in tqdm(lines):\n",
    "        tokens = line.split()\n",
    "        if len(tokens) != 301:\n",
    "            continue\n",
    "        pre_trained_word_embedding[tokens[0]] = np.asarray(tokens[1:]).astype(np.float32)\n",
    "\n",
    "    word_embedding = np.random.uniform(size=(len(word2id), EMBEDDING_DIM))\n",
    "    for key in tqdm(word2id.keys()):\n",
    "        if key in pre_trained_word_embedding:\n",
    "            word_embedding[word2id[key]] = pre_trained_word_embedding[key]\n",
    "\n",
    "    word_embedding[0] = np.zeros(EMBEDDING_DIM)\n",
    "    return torch.from_numpy(word_embedding)\n",
    "\n",
    "logger.info(\"Load Pre-trained Word Embedding...\")\n",
    "word_embedding = load_pre_trained_word_embedding(PRE_TRAINED_WORD_EMBEDDING_FILE_PATH, word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(data, batch_size, word2id, book2genre, is_train=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if is_train:\n",
    "        random.shuffle(indices)\n",
    "\n",
    "    if len(data) % batch_size == 0:\n",
    "        batch_num = int(len(data)/batch_size)\n",
    "    else:\n",
    "        batch_num = int(len(data)/batch_size) + 1\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        left = i*batch_size\n",
    "        right = min((i+1)*batch_size, len(data))\n",
    "\n",
    "        sentences = []\n",
    "        adjacency_matrics = []\n",
    "        labels = []\n",
    "        genres = []\n",
    "\n",
    "        for j in indices[left:right]:\n",
    "            sentence = [word2id[word] for word in data[j][0].split()]\n",
    "            sentence += [0]*(maximum_length - len(sentence))\n",
    "            sentences.append(sentence)\n",
    "            adjacency_matrics.append(data[j][1])\n",
    "            labels.append(data[j][2])\n",
    "            _genres = book2genre[int(data[j][3])]\n",
    "            _genres += [0]*(maximum_genre_length - len(_genres))\n",
    "            genres.append(_genres)\n",
    "\n",
    "        yield sentences, adjacency_matrics, labels, genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Model\n",
    "# ============================\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_dim, self.output_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(self.output_dim))\n",
    "\n",
    "    def forward(self, x, attention_weight):\n",
    "        x = x*attention_weight.unsqueeze(3)\n",
    "        x = x.sum(2)\n",
    "        output = torch.matmul(x, self.weight)\n",
    "        output = output + self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "class GenreEncoder(nn.Module):\n",
    "    def __init__(self, num_filters, filter_sizes, genre_embedding_dim, maximum_genre_length):\n",
    "        super(GenreEncoder, self).__init__()\n",
    "\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.genre_embedding_dim = genre_embedding_dim\n",
    "        self.maximum_genre_length = maximum_genre_length\n",
    "\n",
    "        # ==============================\n",
    "        # 1D CNN\n",
    "        # ==============================\n",
    "        self.cnn = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv1d(self.genre_embedding_dim, self.num_filters, size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.maximum_genre_length - size + 1)\n",
    "        ) for size in self.filter_sizes])\n",
    "\n",
    "\n",
    "    def forward(self, genres):\n",
    "        genres = genres.transpose(1,2)\n",
    "        convs = [conv(genres).squeeze() for conv in self.cnn]\n",
    "\n",
    "        return torch.cat(convs, dim=1)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, alpha, input_dim, output_dim, num_edges, maximum_length):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.maximum_length = maximum_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_edges = num_edges\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # =============================================\n",
    "        # Data Preparation\n",
    "        # =============================================\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_dim, self.output_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "        self.edge_embedding \\\n",
    "        = nn.Embedding(self.num_edges, self.output_dim, padding_idx = 0)\n",
    "        nn.init.xavier_normal_(self.edge_embedding.weight)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, x, adjacency_matrics):\n",
    "        # ==================================\n",
    "        # x: (B, N, H)\n",
    "        # adjacency_matrics: (B, N, N)\n",
    "        hidden = torch.matmul(x, self.weight) # (B, N, H')\n",
    "        hidden = hidden.unsqueeze(1) # (B, 1, N, H')\n",
    "        hidden = hidden.expand(hidden.size(0),\n",
    "                               self.maximum_length,\n",
    "                               self.maximum_length,\n",
    "                               self.output_dim) # (B, N, N, H')\n",
    "\n",
    "        edges = self.edge_embedding(adjacency_matrics)\n",
    "\n",
    "        attention_weight = hidden*edges # (B, N, N, H')\n",
    "        attention_weight = torch.sum(attention_weight, dim=3) # (B, N, N)\n",
    "        attention_weight = self.leakyrelu(attention_weight)\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(attention_weight)\n",
    "        attention_weight = torch.where(adjacency_matrics > 0, attention_weight, zero_vec)\n",
    "        attention_weight = torch.softmax(attention_weight, dim=2)\n",
    "\n",
    "        return attention_weight\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_words,\n",
    "                 num_edges,\n",
    "                 num_genres,\n",
    "                 alpha,\n",
    "                 embedding_dim,\n",
    "                 genre_embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 num_filters,\n",
    "                 filter_sizes,\n",
    "                 maximum_length,\n",
    "                 maximum_genre_length,\n",
    "                 pre_trained,\n",
    "                 dropout_rate):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.num_edges = num_edges\n",
    "        self.num_genres = num_genres\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.genre_embedding_dim = genre_embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.maximum_length = maximum_length\n",
    "        self.maximum_genre_length = maximum_genre_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # =============================================\n",
    "        # Data Preparation\n",
    "        # =============================================\n",
    "        self.word_embedding \\\n",
    "        = nn.Embedding.from_pretrained(pre_trained, freeze=False) # padding_idx = 0,\n",
    "        self.genre_embedding \\\n",
    "        = nn.Embedding(self.num_genres, self.genre_embedding_dim, padding_idx = 0)\n",
    "\n",
    "        self.attention_1 = Attention(self.alpha,\n",
    "                                     2*self.hidden_dim[0],\n",
    "                                     self.hidden_dim[0],\n",
    "                                     self.num_edges,\n",
    "                                     self.maximum_length)\n",
    "\n",
    "        self.attention_2 = Attention(self.alpha,\n",
    "                                     self.hidden_dim[0],\n",
    "                                     self.hidden_dim[1],\n",
    "                                     self.num_edges,\n",
    "                                     self.maximum_length)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim[0], bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.gcn_layer_1 \\\n",
    "        = GCNLayer(2*self.hidden_dim[0], self.hidden_dim[0])\n",
    "        self.gcn_layer_2 \\\n",
    "        = GCNLayer(self.hidden_dim[0], self.hidden_dim[1])\n",
    "\n",
    "        self.genre_encoder = GenreEncoder(self.num_filters,\n",
    "                                          self.filter_sizes,\n",
    "                                          self.genre_embedding_dim,\n",
    "                                          self.maximum_genre_length)\n",
    "\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.hidden_dim[1], self.hidden_dim[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(self.hidden_dim[1], 2)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, sentences, adjacency_matrics, genres):\n",
    "        zero_vec = torch.zeros_like(adjacency_matrics)\n",
    "        adjacency_matrics_t = torch.where(adjacency_matrics > 0,\n",
    "                                          adjacency_matrics + int((self.num_edges-2)/2),\n",
    "                                          zero_vec)\n",
    "        adjacency_matrics_t = adjacency_matrics_t.transpose(1,2)\n",
    "        eye = torch.eye(adjacency_matrics.size(1), dtype=torch.long).cuda()\n",
    "        # eye = torch.eye(adjacency_matrics.size(1), dtype=torch.long)\n",
    "        eye = eye.unsqueeze(0).expand(sentences.size(0),\n",
    "                                      self.maximum_length,\n",
    "                                      self.maximum_length)\n",
    "        adjacency_matrics = adjacency_matrics \\\n",
    "                          + adjacency_matrics_t \\\n",
    "                          + eye # (B, N, N)\n",
    "\n",
    "        embedded_words = self.word_embedding(sentences) # (B, N, D)\n",
    "        h0 = torch.zeros(2, sentences.size(0), self.hidden_dim[0]).cuda() # 2 for bidirection\n",
    "        c0 = torch.zeros(2, sentences.size(0), self.hidden_dim[0]).cuda()\n",
    "        self.lstm.flatten_parameters()\n",
    "        lstm = self.lstm(embedded_words, (h0, c0))[0] # (B, N, 2H)\n",
    "        attention_weight_1 = self.attention_1(lstm, adjacency_matrics)\n",
    "        lstm = lstm.unsqueeze(1)\n",
    "        lstm = lstm.expand(lstm.size(0),\n",
    "                          self.maximum_length,\n",
    "                          self.maximum_length,\n",
    "                          2*self.hidden_dim[0])\n",
    "\n",
    "        gcn_1 = self.gcn_layer_1(lstm, attention_weight_1)\n",
    "        gcn_1 = torch.relu(gcn_1) # B X N X H\n",
    "        gcn_1 = self.dropout(gcn_1)\n",
    "\n",
    "        attention_weight_2 = self.attention_2(gcn_1, adjacency_matrics)\n",
    "        gcn_1 = gcn_1.unsqueeze(1)\n",
    "        gcn_1 = gcn_1.expand(gcn_1.size(0),\n",
    "                          self.maximum_length,\n",
    "                          self.maximum_length,\n",
    "                          self.hidden_dim[0])\n",
    "        gcn_2 = self.gcn_layer_2(gcn_1, attention_weight_2)\n",
    "        gcn_2 = torch.relu(gcn_2) # (B, N, H')\n",
    "\n",
    "        genres = self.genre_embedding(genres) # (B, N', G)\n",
    "        genre_features = self.genre_encoder(genres) # (B, H')\n",
    "        attention_weight_3 = (gcn_2*genre_features.unsqueeze(1)).sum(2) # (B, N)\n",
    "        zero_vec = -9e15*torch.ones_like(attention_weight_3)\n",
    "        attention_weight_3 = torch.where(sentences > 0, attention_weight_3, zero_vec)\n",
    "        attention_weight_3 = torch.softmax(attention_weight_3, dim=1) # (B, N)\n",
    "\n",
    "        sentence_representations = (gcn_2*attention_weight_3.unsqueeze(2)).sum(1) # (B, H')\n",
    "\n",
    "        output = self.output_layer(sentence_representations)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 11:19:42,797 - root <INFO> Model Initializing..\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Model Initialize\n",
    "# =============================================\n",
    "logger.info(\"Model Initializing..\")\n",
    "weight = torch.ones(2).cuda()\n",
    "# weight = torch.ones(2)\n",
    "weight[0] = 0.05\n",
    "criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "\n",
    "model = Model(len(word2id), len(edge2id), len(genre2id),\n",
    "              LEAKY_ALPHA, EMBEDDING_DIM, GENRE_EMBEDDING_DIM, HIDDEN_STATES,\n",
    "              NUM_FILTERS, FILTER_SIZES, maximum_length, maximum_genre_length,\n",
    "              word_embedding.type(\"torch.FloatTensor\"), DROPOUT_RATE)\n",
    "model = nn.DataParallel(model).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del batches\n",
    "# del input_sentences\n",
    "# del input_adjacency_matrics\n",
    "# del input_labels\n",
    "# del input_genres\n",
    "# del logits\n",
    "# del long_logits\n",
    "# del entire_labels\n",
    "# del entire_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-25 11:19:44,951 - root <INFO> \n",
      "Model Training..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [77.5391/0.0798/0.6333/0.1418/0.7721] Loss: 0.5601\n",
      "Epoch: [1/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [81.0547/0.0606/0.6000/0.1101/0.7820] Loss: 0.5300\n",
      "Epoch: [1/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [86.2305/0.1333/0.6452/0.2210/0.8256] Loss: 0.5228\n",
      "Epoch: [1/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.1836/0.1290/0.5517/0.2092/0.8176] Loss: 0.5206\n",
      "Epoch: [1/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [86.8164/0.0963/0.5000/0.1615/0.8608] Loss: 0.5177\n",
      "Epoch: [1/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.3516/0.1083/0.6296/0.1848/0.7728] Loss: 0.5111\n",
      "Epoch: [1/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [84.4727/0.1104/0.5625/0.1846/0.7794] Loss: 0.5097\n",
      "Epoch: [1/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [84.9609/0.0789/0.4615/0.1348/0.7215] Loss: 0.5084\n",
      "Epoch: [1/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.0117/0.1538/0.6471/0.2486/0.9038] Loss: 0.5066\n",
      "Epoch: [1/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.7027/0.0960/0.5387/0.1629/0.8000] Loss: 497.9928\n",
      "Epoch: [2/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.3047/0.1397/0.5938/0.2262/0.7971] Loss: 0.4879\n",
      "Epoch: [2/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [90.4297/0.1250/0.4615/0.1967/0.8521] Loss: 0.4897\n",
      "Epoch: [2/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [86.5234/0.1429/0.6364/0.2333/0.8158] Loss: 0.4925\n",
      "Epoch: [2/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [80.4688/0.0452/0.4737/0.0826/0.7245] Loss: 0.4927\n",
      "Epoch: [2/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.0586/0.0705/0.5789/0.1257/0.7898] Loss: 0.4919\n",
      "Epoch: [2/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.2070/0.0827/0.5500/0.1438/0.8291] Loss: 0.4917\n",
      "Epoch: [2/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.6953/0.1439/0.5938/0.2317/0.8414] Loss: 0.4890\n",
      "Epoch: [2/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [81.5430/0.0741/0.5000/0.1290/0.8149] Loss: 0.4914\n",
      "Epoch: [2/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.3750/0.0898/0.6522/0.1579/0.8227] Loss: 0.4894\n",
      "Epoch: [2/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [84.2791/0.0929/0.5805/0.1602/0.8093] Loss: 484.5703\n",
      "Current Best:)\n",
      "Test Batch: [2527/2527]09-25_11:19_only_attention2 [A/P/R/F/ROC]: [84.3642/0.0964/0.5764/0.1652/0.8129]\n",
      "Epoch: [3/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.8164/0.1022/0.5385/0.1718/0.8299] Loss: 0.4732\n",
      "Epoch: [3/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [85.3516/0.1203/0.6333/0.2021/0.8487] Loss: 0.4803\n",
      "Epoch: [3/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.3516/0.1019/0.6400/0.1758/0.8331] Loss: 0.4758\n",
      "Epoch: [3/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.5234/0.1517/0.5946/0.2418/0.8479] Loss: 0.4795\n",
      "Epoch: [3/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.6445/0.0962/0.7143/0.1695/0.8993] Loss: 0.4758\n",
      "Epoch: [3/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.5234/0.1151/0.5161/0.1882/0.7790] Loss: 0.4801\n",
      "Epoch: [3/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.5977/0.1290/0.4571/0.2013/0.8679] Loss: 0.4799\n",
      "Epoch: [3/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [85.0586/0.0875/0.6667/0.1547/0.8604] Loss: 0.4798\n",
      "Epoch: [3/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.6953/0.1163/0.5556/0.1923/0.8153] Loss: 0.4806\n",
      "Epoch: [3/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.3626/0.1024/0.5511/0.1727/0.8152] Loss: 475.0809\n",
      "Current Best:)\n",
      "Test Batch: [2527/2527]09-25_11:19_only_attention3 [A/P/R/F/ROC]: [86.3535/0.1054/0.5453/0.1766/0.8145]\n",
      "Epoch: [4/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [84.3750/0.1024/0.6071/0.1753/0.8136] Loss: 0.4613\n",
      "Epoch: [4/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [84.1797/0.1059/0.6429/0.1818/0.8492] Loss: 0.4673\n",
      "Epoch: [4/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [83.5938/0.0843/0.4667/0.1429/0.7920] Loss: 0.4714\n",
      "Epoch: [4/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.1836/0.1000/0.4800/0.1655/0.8542] Loss: 0.4713\n",
      "Epoch: [4/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [87.2070/0.1293/0.8636/0.2249/0.8921] Loss: 0.4737\n",
      "Epoch: [4/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [88.9648/0.1600/0.7143/0.2614/0.8988] Loss: 0.4719\n",
      "Epoch: [4/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [88.4766/0.1441/0.5000/0.2237/0.7957] Loss: 0.4752\n",
      "Epoch: [4/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [83.7891/0.0819/0.6087/0.1443/0.8679] Loss: 0.4719\n",
      "Epoch: [4/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.6680/0.1084/0.6667/0.1865/0.8730] Loss: 0.4730\n",
      "Epoch: [4/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [84.9775/0.0957/0.5698/0.1638/0.8130] Loss: 467.2266\n",
      "Epoch: [5/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.8164/0.1458/0.6364/0.2373/0.8591] Loss: 0.4564\n",
      "Epoch: [5/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.2305/0.1284/0.6129/0.2123/0.8600] Loss: 0.4598\n",
      "Epoch: [5/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [87.1094/0.0942/0.6500/0.1646/0.8464] Loss: 0.4647\n",
      "Epoch: [5/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.0117/0.1854/0.7368/0.2963/0.8984] Loss: 0.4640\n",
      "Epoch: [5/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.7422/0.1250/0.4737/0.1978/0.8110] Loss: 0.4625\n",
      "Epoch: [5/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.0352/0.1184/0.6667/0.2011/0.8598] Loss: 0.4697\n",
      "Epoch: [5/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [85.7422/0.1176/0.6207/0.1978/0.8549] Loss: 0.4679\n",
      "Epoch: [5/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [89.2578/0.1681/0.6452/0.2667/0.8569] Loss: 0.4677\n",
      "Epoch: [5/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.4023/0.1471/0.6061/0.2367/0.8341] Loss: 0.4697\n",
      "Epoch: [5/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.1914/0.0987/0.5342/0.1665/0.8044] Loss: 464.9737\n",
      "Epoch: [6/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [85.6445/0.1234/0.6129/0.2054/0.8378] Loss: 0.4489\n",
      "Epoch: [6/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [85.2539/0.0886/0.6667/0.1564/0.8519] Loss: 0.4532\n",
      "Epoch: [6/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.4766/0.1000/0.3667/0.1571/0.7938] Loss: 0.4574\n",
      "Epoch: [6/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.0859/0.1240/0.6400/0.2078/0.8313] Loss: 0.4579\n",
      "Epoch: [6/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.5469/0.1242/0.5758/0.2043/0.8411] Loss: 0.4598\n",
      "Epoch: [6/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [89.4531/0.1284/0.5185/0.2059/0.8497] Loss: 0.4655\n",
      "Epoch: [6/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [84.2773/0.0920/0.5357/0.1571/0.7928] Loss: 0.4648\n",
      "Epoch: [6/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.9141/0.0863/0.6316/0.1519/0.8460] Loss: 0.4641\n",
      "Epoch: [6/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [86.0352/0.1156/0.5667/0.1921/0.8101] Loss: 0.4655\n",
      "Epoch: [6/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [87.4631/0.1075/0.5275/0.1786/0.8137] Loss: 462.3132\n",
      "Epoch: [7/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [85.5469/0.1242/0.5758/0.2043/0.8387] Loss: 0.4419\n",
      "Epoch: [7/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.3047/0.1119/0.5769/0.1875/0.8198] Loss: 0.4497\n",
      "Epoch: [7/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [86.9141/0.0833/0.4583/0.1410/0.7747] Loss: 0.4541\n",
      "Epoch: [7/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.3047/0.1250/0.6071/0.2073/0.8323] Loss: 0.4552\n",
      "Epoch: [7/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.0586/0.1288/0.6562/0.2154/0.8545] Loss: 0.4592\n",
      "Epoch: [7/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.5469/0.1126/0.5484/0.1868/0.7540] Loss: 0.4596\n",
      "Epoch: [7/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.8906/0.0930/0.6316/0.1622/0.8831] Loss: 0.4603\n",
      "Epoch: [7/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.6211/0.1143/0.5517/0.1893/0.8207] Loss: 0.4654\n",
      "Epoch: [7/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [90.9180/0.1250/0.5714/0.2051/0.8588] Loss: 0.4599\n",
      "Epoch: [7/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.8021/0.0981/0.5491/0.1665/0.8096] Loss: 459.1630\n",
      "Epoch: [8/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.4258/0.1156/0.6538/0.1965/0.8408] Loss: 0.4382\n",
      "Epoch: [8/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [90.4297/0.1400/0.5385/0.2222/0.8554] Loss: 0.4439\n",
      "Epoch: [8/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [84.9609/0.0988/0.6667/0.1720/0.8635] Loss: 0.4476\n",
      "Epoch: [8/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.2305/0.1049/0.5357/0.1754/0.8315] Loss: 0.4523\n",
      "Epoch: [8/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [89.2578/0.1604/0.4474/0.2361/0.7876] Loss: 0.4540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.3047/0.1212/0.5333/0.1975/0.8339] Loss: 0.4576\n",
      "Epoch: [8/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [85.2539/0.0784/0.5455/0.1371/0.8037] Loss: 0.4578\n",
      "Epoch: [8/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [83.2031/0.0955/0.6071/0.1650/0.8174] Loss: 0.4578\n",
      "Epoch: [8/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [88.5742/0.1280/0.6667/0.2148/0.9003] Loss: 0.4618\n",
      "Epoch: [8/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.1540/0.1031/0.5665/0.1745/0.8163] Loss: 457.9049\n",
      "Current Best:)\n",
      "Test Batch: [2527/2527]09-25_11:19_only_attention8 [A/P/R/F/ROC]: [86.3096/0.1058/0.5505/0.1775/0.8146]\n",
      "Epoch: [9/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.5977/0.1397/0.6552/0.2303/0.8532] Loss: 0.4367\n",
      "Epoch: [9/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [89.7461/0.1364/0.6000/0.2222/0.8503] Loss: 0.4429\n",
      "Epoch: [9/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.9648/0.1176/0.6364/0.1986/0.8505] Loss: 0.4477\n",
      "Epoch: [9/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [89.6484/0.1624/0.7037/0.2639/0.8940] Loss: 0.4495\n",
      "Epoch: [9/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [84.1797/0.1186/0.7778/0.2059/0.8529] Loss: 0.4513\n",
      "Epoch: [9/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.3281/0.1241/0.5806/0.2045/0.8300] Loss: 0.4558\n",
      "Epoch: [9/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.9141/0.1087/0.5769/0.1829/0.8361] Loss: 0.4561\n",
      "Epoch: [9/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [89.2578/0.1429/0.6800/0.2361/0.8970] Loss: 0.4582\n",
      "Epoch: [9/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [85.8398/0.1192/0.6000/0.1989/0.7924] Loss: 0.4548\n",
      "Epoch: [9/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [84.0214/0.0936/0.5971/0.1618/0.8181] Loss: 453.2342\n",
      "Current Best:)\n",
      "Test Batch: [2527/2527]09-25_11:19_only_attention9 [A/P/R/F/ROC]: [84.2415/0.0972/0.5880/0.1669/0.8147]\n",
      "Epoch: [10/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.1094/0.1135/0.6957/0.1951/0.8830] Loss: 0.4350\n",
      "Epoch: [10/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.1094/0.1103/0.5769/0.1852/0.8577] Loss: 0.4384\n",
      "Epoch: [10/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [83.6914/0.1067/0.7037/0.1854/0.8657] Loss: 0.4442\n",
      "Epoch: [10/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [85.1562/0.1145/0.7917/0.2000/0.9145] Loss: 0.4447\n",
      "Epoch: [10/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.8672/0.1140/0.5000/0.1857/0.8134] Loss: 0.4514\n",
      "Epoch: [10/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.5000/0.1176/0.6667/0.2000/0.8561] Loss: 0.4517\n",
      "Epoch: [10/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [85.5469/0.0909/0.4194/0.1494/0.7473] Loss: 0.4537\n",
      "Epoch: [10/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.6211/0.1401/0.9167/0.2431/0.9050] Loss: 0.4554\n",
      "Epoch: [10/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [89.2578/0.1750/0.6562/0.2763/0.8498] Loss: 0.4560\n",
      "Epoch: [10/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.6246/0.1022/0.5366/0.1717/0.8135] Loss: 454.5177\n",
      "Epoch: [11/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [88.6719/0.1653/0.5714/0.2564/0.8614] Loss: 0.4287\n",
      "Epoch: [11/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.8164/0.0752/0.4545/0.1290/0.8689] Loss: 0.4390\n",
      "Epoch: [11/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.0859/0.1138/0.5185/0.1867/0.8486] Loss: 0.4397\n",
      "Epoch: [11/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.2070/0.1399/0.7143/0.2339/0.8832] Loss: 0.4441\n",
      "Epoch: [11/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [86.9141/0.1460/0.5405/0.2299/0.8329] Loss: 0.4480\n",
      "Epoch: [11/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.1328/0.0780/0.4783/0.1341/0.8554] Loss: 0.4487\n",
      "Epoch: [11/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.4023/0.1333/0.6000/0.2182/0.8733] Loss: 0.4544\n",
      "Epoch: [11/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [85.7422/0.1226/0.6552/0.2065/0.7927] Loss: 0.4545\n",
      "Epoch: [11/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.2070/0.1045/0.5600/0.1761/0.8454] Loss: 0.4546\n",
      "Epoch: [11/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.9989/0.0995/0.5495/0.1686/0.8129] Loss: 450.4504\n",
      "Epoch: [12/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.1328/0.1111/0.7391/0.1932/0.8854] Loss: 0.4288\n",
      "Epoch: [12/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [84.8633/0.1243/0.7500/0.2132/0.9058] Loss: 0.4375\n",
      "Epoch: [12/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.7422/0.1097/0.6800/0.1889/0.8958] Loss: 0.4385\n",
      "Epoch: [12/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [83.1055/0.1340/0.8387/0.2311/0.8952] Loss: 0.4443\n",
      "Epoch: [12/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.1562/0.1765/0.7143/0.2830/0.8405] Loss: 0.4445\n",
      "Epoch: [12/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.1562/0.1325/0.7333/0.2245/0.8812] Loss: 0.4483\n",
      "Epoch: [12/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.5977/0.1298/0.5667/0.2112/0.8429] Loss: 0.4517\n",
      "Epoch: [12/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.5234/0.0986/0.5833/0.1687/0.8588] Loss: 0.4479\n",
      "Epoch: [12/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.9883/0.1364/0.6667/0.2264/0.8511] Loss: 0.4533\n",
      "Epoch: [12/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.7561/0.1003/0.5665/0.1704/0.8198] Loss: 447.6035\n",
      "Current Best:)\n",
      "Test Batch: [2527/2527]09-25_11:19_only_attention12 [A/P/R/F/ROC]: [85.8589/0.1034/0.5567/0.1744/0.8148]\n",
      "Epoch: [13/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [82.8125/0.1140/0.8148/0.2000/0.9004] Loss: 0.4263\n",
      "Epoch: [13/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [81.8359/0.0938/0.6000/0.1622/0.8616] Loss: 0.4337\n",
      "Epoch: [13/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.7422/0.0921/0.6364/0.1609/0.8104] Loss: 0.4369\n",
      "Epoch: [13/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [84.9609/0.0955/0.5556/0.1630/0.7888] Loss: 0.4424\n",
      "Epoch: [13/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.7695/0.1356/0.5517/0.2177/0.8371] Loss: 0.4421\n",
      "Epoch: [13/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [89.4531/0.1638/0.6333/0.2603/0.8688] Loss: 0.4466\n",
      "Epoch: [13/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [85.0586/0.0596/0.4500/0.1053/0.8443] Loss: 0.4482\n",
      "Epoch: [13/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [84.3750/0.1517/0.7500/0.2523/0.8697] Loss: 0.4470\n",
      "Epoch: [13/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.6680/0.1145/0.6552/0.1949/0.8311] Loss: 0.4497\n",
      "Epoch: [13/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [83.5914/0.0890/0.5797/0.1543/0.8070] Loss: 446.4588\n",
      "Epoch: [14/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.3281/0.1361/0.6061/0.2222/0.8722] Loss: 0.4191\n",
      "Epoch: [14/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [81.9336/0.0808/0.8421/0.1475/0.8987] Loss: 0.4297\n",
      "Epoch: [14/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [86.1328/0.1569/0.6486/0.2526/0.8703] Loss: 0.4359\n",
      "Epoch: [14/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [89.2578/0.2105/0.8485/0.3373/0.9292] Loss: 0.4384\n",
      "Epoch: [14/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.8398/0.0426/0.3750/0.0764/0.8042] Loss: 0.4421\n",
      "Epoch: [14/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [84.6680/0.1118/0.5625/0.1865/0.7819] Loss: 0.4450\n",
      "Epoch: [14/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [83.8867/0.1130/0.7143/0.1951/0.8510] Loss: 0.4452\n",
      "Epoch: [14/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [88.3789/0.1395/0.6923/0.2323/0.9035] Loss: 0.4466\n",
      "Epoch: [14/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [86.8164/0.0993/0.6364/0.1718/0.8521] Loss: 0.4468\n",
      "Epoch: [14/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [81.5465/0.0859/0.6377/0.1515/0.8157] Loss: 449.8754\n",
      "Epoch: [15/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [89.7461/0.1468/0.5714/0.2336/0.8159] Loss: 0.4190\n",
      "Epoch: [15/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.5977/0.1736/0.7576/0.2825/0.9212] Loss: 0.4255\n",
      "Epoch: [15/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [89.0625/0.1552/0.5625/0.2432/0.8634] Loss: 0.4327\n",
      "Epoch: [15/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [83.3008/0.1264/0.6571/0.2120/0.8336] Loss: 0.4358\n",
      "Epoch: [15/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [86.2305/0.0429/0.4615/0.0784/0.8137] Loss: 0.4393\n",
      "Epoch: [15/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [84.2773/0.1220/0.5405/0.1990/0.7860] Loss: 0.4416\n",
      "Epoch: [15/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [89.9414/0.1885/0.8519/0.3087/0.9288] Loss: 0.4463\n",
      "Epoch: [15/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [85.0586/0.0988/0.6957/0.1730/0.8737] Loss: 0.4447\n",
      "Epoch: [15/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.8906/0.1280/0.5161/0.2051/0.8482] Loss: 0.4488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [88.2920/0.1089/0.4919/0.1783/0.8095] Loss: 446.5270\n",
      "Epoch: [16/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [82.6172/0.0718/0.5652/0.1275/0.7745] Loss: 0.4152\n",
      "Epoch: [16/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.9141/0.0846/0.4231/0.1410/0.8514] Loss: 0.4239\n",
      "Epoch: [16/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [87.1094/0.1151/0.6400/0.1951/0.8894] Loss: 0.4301\n",
      "Epoch: [16/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [84.2773/0.0994/0.7083/0.1744/0.8910] Loss: 0.4357\n",
      "Epoch: [16/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.4492/0.1062/0.7391/0.1858/0.8915] Loss: 0.4385\n",
      "Epoch: [16/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.7188/0.1477/0.7097/0.2444/0.8961] Loss: 0.4422\n",
      "Epoch: [16/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.6211/0.1429/0.6562/0.2346/0.8938] Loss: 0.4437\n",
      "Epoch: [16/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [88.2812/0.1077/0.7778/0.1892/0.9219] Loss: 0.4428\n",
      "Epoch: [16/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [91.1133/0.1600/0.6957/0.2602/0.8991] Loss: 0.4478\n",
      "Epoch: [16/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [82.5572/0.0883/0.6166/0.1544/0.8141] Loss: 442.7888\n",
      "Epoch: [17/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.3281/0.1667/0.7222/0.2708/0.8840] Loss: 0.4161\n",
      "Epoch: [17/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.5977/0.1667/0.6571/0.2659/0.9081] Loss: 0.4249\n",
      "Epoch: [17/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [84.8633/0.0584/0.4737/0.1040/0.8321] Loss: 0.4263\n",
      "Epoch: [17/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [83.0078/0.1087/0.6667/0.1869/0.8498] Loss: 0.4326\n",
      "Epoch: [17/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.9648/0.1870/0.6389/0.2893/0.8349] Loss: 0.4377\n",
      "Epoch: [17/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.7422/0.1088/0.5161/0.1798/0.8432] Loss: 0.4386\n",
      "Epoch: [17/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.8164/0.1007/0.5833/0.1718/0.8218] Loss: 0.4393\n",
      "Epoch: [17/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.4258/0.1000/0.5185/0.1677/0.8378] Loss: 0.4421\n",
      "Epoch: [17/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [86.5234/0.1389/0.5882/0.2247/0.8338] Loss: 0.4460\n",
      "Epoch: [17/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [84.8952/0.0975/0.5872/0.1672/0.8144] Loss: 441.7922\n",
      "Epoch: [18/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.0117/0.1310/0.7308/0.2222/0.8767] Loss: 0.4145\n",
      "Epoch: [18/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.1094/0.1206/0.6800/0.2048/0.8840] Loss: 0.4167\n",
      "Epoch: [18/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [86.9141/0.1479/0.6176/0.2386/0.8426] Loss: 0.4254\n",
      "Epoch: [18/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.6953/0.1111/0.5000/0.1818/0.8220] Loss: 0.4324\n",
      "Epoch: [18/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.7422/0.1126/0.5862/0.1889/0.8425] Loss: 0.4351\n",
      "Epoch: [18/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [83.7891/0.0852/0.7500/0.1531/0.8724] Loss: 0.4385\n",
      "Epoch: [18/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [83.3008/0.0924/0.8095/0.1659/0.8914] Loss: 0.4414\n",
      "Epoch: [18/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [84.5703/0.1337/0.7188/0.2255/0.8658] Loss: 0.4425\n",
      "Epoch: [18/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.0117/0.1172/0.7727/0.2036/0.8880] Loss: 0.4427\n",
      "Epoch: [18/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [82.3947/0.0882/0.6228/0.1545/0.8143] Loss: 441.1420\n",
      "Epoch: [19/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [88.7695/0.1250/0.6000/0.2069/0.9046] Loss: 0.4087\n",
      "Epoch: [19/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [83.8867/0.0977/0.6800/0.1709/0.8563] Loss: 0.4190\n",
      "Epoch: [19/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.7422/0.0616/0.5000/0.1098/0.7978] Loss: 0.4262\n",
      "Epoch: [19/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.0859/0.1462/0.6333/0.2375/0.8922] Loss: 0.4283\n",
      "Epoch: [19/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.3789/0.0847/0.4762/0.1439/0.8542] Loss: 0.4316\n",
      "Epoch: [19/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.6211/0.1467/0.7097/0.2431/0.8691] Loss: 0.4326\n",
      "Epoch: [19/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [88.8672/0.1311/0.6667/0.2192/0.9078] Loss: 0.4390\n",
      "Epoch: [19/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [87.8906/0.1714/0.7500/0.2791/0.9125] Loss: 0.4391\n",
      "Epoch: [19/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [89.2578/0.0982/0.5500/0.1667/0.8834] Loss: 0.4449\n",
      "Epoch: [19/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.8941/0.1007/0.5623/0.1708/0.8119] Loss: 440.5390\n",
      "Epoch: [20/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [85.1562/0.1375/0.6111/0.2245/0.8607] Loss: 0.4068\n",
      "Epoch: [20/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.0117/0.1565/0.7188/0.2570/0.8900] Loss: 0.4160\n",
      "Epoch: [20/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.1562/0.1195/0.6129/0.2000/0.8571] Loss: 0.4205\n",
      "Epoch: [20/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.4766/0.1860/0.6486/0.2892/0.8690] Loss: 0.4279\n",
      "Epoch: [20/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [84.1797/0.0769/0.6842/0.1383/0.8439] Loss: 0.4315\n",
      "Epoch: [20/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.9883/0.1357/0.9048/0.2360/0.9368] Loss: 0.4346\n",
      "Epoch: [20/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.6953/0.1353/0.6207/0.2222/0.8690] Loss: 0.4382\n",
      "Epoch: [20/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [82.6172/0.1042/0.7692/0.1835/0.9042] Loss: 0.4400\n",
      "Epoch: [20/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.1797/0.1314/0.6970/0.2212/0.8723] Loss: 0.4413\n",
      "Epoch: [20/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.2802/0.0988/0.5785/0.1688/0.8127] Loss: 439.8273\n",
      "Epoch: [21/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.8164/0.1361/0.7143/0.2286/0.9070] Loss: 0.4050\n",
      "Epoch: [21/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [85.8398/0.1067/0.5926/0.1808/0.8309] Loss: 0.4148\n",
      "Epoch: [21/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [89.1602/0.1545/0.7308/0.2550/0.9045] Loss: 0.4189\n",
      "Epoch: [21/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [83.6914/0.0930/0.5926/0.1608/0.8467] Loss: 0.4257\n",
      "Epoch: [21/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [84.3750/0.1235/0.6562/0.2079/0.8701] Loss: 0.4310\n",
      "Epoch: [21/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.1562/0.1472/0.6486/0.2400/0.8547] Loss: 0.4358\n",
      "Epoch: [21/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.8906/0.1397/0.7308/0.2346/0.8864] Loss: 0.4379\n",
      "Epoch: [21/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [89.2578/0.1500/0.6923/0.2466/0.8768] Loss: 0.4380\n",
      "Epoch: [21/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.7656/0.1159/0.6333/0.1959/0.8623] Loss: 0.4403\n",
      "Epoch: [21/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.0160/0.0998/0.5507/0.1690/0.8122] Loss: 435.9929\n",
      "Epoch: [22/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [85.8398/0.1329/0.7241/0.2246/0.8905] Loss: 0.4027\n",
      "Epoch: [22/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [85.3516/0.1447/0.6216/0.2347/0.8485] Loss: 0.4143\n",
      "Epoch: [22/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.6445/0.0980/0.6250/0.1695/0.8679] Loss: 0.4183\n",
      "Epoch: [22/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.8672/0.1600/0.6897/0.2597/0.8970] Loss: 0.4234\n",
      "Epoch: [22/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [84.1797/0.1264/0.8846/0.2212/0.9065] Loss: 0.4285\n",
      "Epoch: [22/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.0352/0.0833/0.5217/0.1437/0.7959] Loss: 0.4346\n",
      "Epoch: [22/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [83.4961/0.0739/0.6842/0.1333/0.8915] Loss: 0.4370\n",
      "Epoch: [22/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [89.3555/0.1475/0.7826/0.2483/0.8827] Loss: 0.4377\n",
      "Epoch: [22/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [89.2578/0.1721/0.7000/0.2763/0.8629] Loss: 0.4378\n",
      "Epoch: [22/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [82.7604/0.0883/0.6087/0.1543/0.8122] Loss: 435.1630\n",
      "Epoch: [23/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [89.0625/0.1356/0.6154/0.2222/0.8779] Loss: 0.3986\n",
      "Epoch: [23/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [85.7422/0.1141/0.5484/0.1889/0.8202] Loss: 0.4101\n",
      "Epoch: [23/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.2539/0.1598/0.7500/0.2634/0.8705] Loss: 0.4152\n",
      "Epoch: [23/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.0352/0.1346/0.7241/0.2270/0.8798] Loss: 0.4233\n",
      "Epoch: [23/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [86.4258/0.1250/0.7600/0.2147/0.9001] Loss: 0.4262\n",
      "Epoch: [23/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.6953/0.1690/0.7500/0.2759/0.8966] Loss: 0.4297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [23/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [85.0586/0.1019/0.5714/0.1730/0.8139] Loss: 0.4336\n",
      "Epoch: [23/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [85.7422/0.1290/0.6452/0.2151/0.8389] Loss: 0.4351\n",
      "Epoch: [23/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [85.8398/0.0909/0.7368/0.1618/0.8914] Loss: 0.4400\n",
      "Epoch: [23/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.9636/0.1025/0.5217/0.1713/0.8065] Loss: 437.2518\n",
      "Epoch: [24/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.1328/0.1032/0.8421/0.1839/0.9265] Loss: 0.3964\n",
      "Epoch: [24/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.6211/0.1241/0.6429/0.2081/0.9063] Loss: 0.4092\n",
      "Epoch: [24/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.7695/0.1587/0.6897/0.2581/0.8732] Loss: 0.4156\n",
      "Epoch: [24/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.6211/0.1533/0.6970/0.2514/0.8986] Loss: 0.4213\n",
      "Epoch: [24/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.9648/0.1600/0.7143/0.2614/0.8862] Loss: 0.4253\n",
      "Epoch: [24/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [89.1602/0.1746/0.7586/0.2839/0.9120] Loss: 0.4302\n",
      "Epoch: [24/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.9141/0.1151/0.5926/0.1928/0.8344] Loss: 0.4309\n",
      "Epoch: [24/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.0352/0.1592/0.6944/0.2591/0.9012] Loss: 0.4334\n",
      "Epoch: [24/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.6680/0.1243/0.7000/0.2111/0.8782] Loss: 0.4389\n",
      "Epoch: [24/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [88.5872/0.1130/0.4994/0.1844/0.8085] Loss: 434.3027\n",
      "Epoch: [25/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [88.2812/0.1630/0.7586/0.2683/0.8971] Loss: 0.3966\n",
      "Epoch: [25/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.8164/0.1622/0.6857/0.2623/0.8862] Loss: 0.4047\n",
      "Epoch: [25/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [87.5000/0.1571/0.6875/0.2558/0.8715] Loss: 0.4155\n",
      "Epoch: [25/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.7188/0.1223/0.5484/0.2000/0.8337] Loss: 0.4180\n",
      "Epoch: [25/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.5742/0.1008/0.5455/0.1702/0.8530] Loss: 0.4244\n",
      "Epoch: [25/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.7188/0.1268/0.6000/0.2093/0.8465] Loss: 0.4273\n",
      "Epoch: [25/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.0117/0.1049/0.7500/0.1840/0.8298] Loss: 0.4328\n",
      "Epoch: [25/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.0352/0.1275/0.5938/0.2099/0.8685] Loss: 0.4335\n",
      "Epoch: [25/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [89.1602/0.1282/0.6250/0.2128/0.8974] Loss: 0.4325\n",
      "Epoch: [25/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.8107/0.0985/0.5516/0.1672/0.8108] Loss: 432.9875\n",
      "Epoch: [26/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [85.9375/0.1392/0.7333/0.2340/0.8822] Loss: 0.3955\n",
      "Epoch: [26/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.5000/0.1429/0.7143/0.2381/0.8699] Loss: 0.4049\n",
      "Epoch: [26/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.7422/0.1350/0.8148/0.2316/0.8865] Loss: 0.4108\n",
      "Epoch: [26/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.6211/0.1548/0.8000/0.2595/0.9320] Loss: 0.4169\n",
      "Epoch: [26/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [90.4297/0.1545/0.7727/0.2576/0.9319] Loss: 0.4221\n",
      "Epoch: [26/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.5469/0.1218/0.6333/0.2043/0.8852] Loss: 0.4276\n",
      "Epoch: [26/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [85.4492/0.1304/0.7000/0.2199/0.8467] Loss: 0.4301\n",
      "Epoch: [26/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [88.3789/0.1926/0.7222/0.3041/0.8902] Loss: 0.4326\n",
      "Epoch: [26/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.7930/0.1145/0.6250/0.1935/0.8448] Loss: 0.4339\n",
      "Epoch: [26/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [82.1904/0.0840/0.5954/0.1473/0.8089] Loss: 431.5385\n",
      "Epoch: [27/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.3047/0.0833/0.5500/0.1447/0.8500] Loss: 0.3908\n",
      "Epoch: [27/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.0117/0.1622/0.7273/0.2652/0.8736] Loss: 0.4023\n",
      "Epoch: [27/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [89.4531/0.0982/0.6111/0.1692/0.8605] Loss: 0.4107\n",
      "Epoch: [27/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [89.8438/0.1504/0.6800/0.2464/0.9080] Loss: 0.4142\n",
      "Epoch: [27/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [87.4023/0.0985/0.5652/0.1677/0.8558] Loss: 0.4219\n",
      "Epoch: [27/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.1094/0.0857/0.7500/0.1538/0.8553] Loss: 0.4257\n",
      "Epoch: [27/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.3281/0.1216/0.6429/0.2045/0.8587] Loss: 0.4275\n",
      "Epoch: [27/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [85.2539/0.1210/0.5938/0.2011/0.8358] Loss: 0.4315\n",
      "Epoch: [27/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.6680/0.0793/0.6842/0.1421/0.8499] Loss: 0.4319\n",
      "Epoch: [27/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.0866/0.0982/0.5362/0.1660/0.8062] Loss: 434.9283\n",
      "Epoch: [28/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.8906/0.1240/0.5926/0.2051/0.8560] Loss: 0.3901\n",
      "Epoch: [28/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [84.1797/0.1285/0.7931/0.2212/0.8942] Loss: 0.3986\n",
      "Epoch: [28/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [87.0117/0.1324/0.5455/0.2130/0.8484] Loss: 0.4097\n",
      "Epoch: [28/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.8164/0.1538/0.6111/0.2458/0.8635] Loss: 0.4153\n",
      "Epoch: [28/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.2539/0.1329/0.6000/0.2176/0.8496] Loss: 0.4207\n",
      "Epoch: [28/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [88.2812/0.1154/0.7500/0.2000/0.8895] Loss: 0.4215\n",
      "Epoch: [28/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.5977/0.1567/0.6000/0.2485/0.8690] Loss: 0.4281\n",
      "Epoch: [28/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [82.8125/0.1385/0.7714/0.2348/0.8621] Loss: 0.4289\n",
      "Epoch: [28/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [88.8672/0.0893/0.4545/0.1493/0.8596] Loss: 0.4348\n",
      "Epoch: [28/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [82.7112/0.0875/0.6037/0.1528/0.8100] Loss: 430.0825\n",
      "Epoch: [29/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [88.3789/0.1450/0.7308/0.2420/0.9212] Loss: 0.3909\n",
      "Epoch: [29/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [89.0625/0.1475/0.6923/0.2432/0.9106] Loss: 0.3966\n",
      "Epoch: [29/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.1562/0.0839/0.5652/0.1461/0.8393] Loss: 0.4048\n",
      "Epoch: [29/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [82.1289/0.1408/0.8286/0.2407/0.8824] Loss: 0.4111\n",
      "Epoch: [29/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.0859/0.1085/0.6667/0.1867/0.8634] Loss: 0.4192\n",
      "Epoch: [29/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [86.1328/0.0915/0.8235/0.1647/0.8819] Loss: 0.4208\n",
      "Epoch: [29/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.1094/0.1399/0.6897/0.2326/0.8990] Loss: 0.4272\n",
      "Epoch: [29/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [88.7695/0.1138/0.7000/0.1958/0.8689] Loss: 0.4284\n",
      "Epoch: [29/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.9883/0.1016/0.6190/0.1745/0.8777] Loss: 0.4312\n",
      "Epoch: [29/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.9872/0.1042/0.5317/0.1743/0.8122] Loss: 431.5065\n",
      "Epoch: [30/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.2070/0.1439/0.5135/0.2249/0.8580] Loss: 0.3833\n",
      "Epoch: [30/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [84.5703/0.1053/0.7826/0.1856/0.9206] Loss: 0.3930\n",
      "Epoch: [30/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [86.3281/0.1046/0.8421/0.1860/0.9301] Loss: 0.4055\n",
      "Epoch: [30/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.0352/0.1282/0.7407/0.2186/0.9034] Loss: 0.4113\n",
      "Epoch: [30/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [84.8633/0.1341/0.6286/0.2211/0.8406] Loss: 0.4165\n",
      "Epoch: [30/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.3047/0.1883/0.8529/0.3085/0.9039] Loss: 0.4188\n",
      "Epoch: [30/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.1328/0.1307/0.6897/0.2198/0.8706] Loss: 0.4282\n",
      "Epoch: [30/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [82.4219/0.1212/0.8000/0.2105/0.8660] Loss: 0.4290\n",
      "Epoch: [30/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [84.9609/0.1019/0.5517/0.1720/0.8441] Loss: 0.4318\n",
      "Epoch: [30/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [84.2684/0.0930/0.5814/0.1603/0.8105] Loss: 431.1048\n",
      "Epoch: [31/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [89.5508/0.1500/0.7826/0.2517/0.9390] Loss: 0.3819\n",
      "Epoch: [31/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.9141/0.1295/0.5806/0.2118/0.8345] Loss: 0.3964\n",
      "Epoch: [31/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.8672/0.1508/0.7308/0.2500/0.8999] Loss: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [31/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.2070/0.1324/0.5806/0.2156/0.8801] Loss: 0.4100\n",
      "Epoch: [31/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [86.3281/0.1625/0.8125/0.2708/0.9235] Loss: 0.4148\n",
      "Epoch: [31/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [89.0625/0.1694/0.7000/0.2727/0.8736] Loss: 0.4194\n",
      "Epoch: [31/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.8164/0.1103/0.7273/0.1916/0.9032] Loss: 0.4270\n",
      "Epoch: [31/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [84.6680/0.1310/0.6667/0.2189/0.8516] Loss: 0.4263\n",
      "Epoch: [31/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [82.0312/0.0632/0.6667/0.1154/0.8327] Loss: 0.4314\n",
      "Epoch: [31/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [87.2781/0.1046/0.5193/0.1741/0.8095] Loss: 429.1055\n",
      "Epoch: [32/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.0117/0.1765/0.7941/0.2888/0.9177] Loss: 0.3789\n",
      "Epoch: [32/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [88.3789/0.1550/0.6667/0.2516/0.9011] Loss: 0.3906\n",
      "Epoch: [32/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.7695/0.1271/0.5556/0.2069/0.8494] Loss: 0.3998\n",
      "Epoch: [32/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.4258/0.1250/0.7600/0.2147/0.9017] Loss: 0.4119\n",
      "Epoch: [32/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.6445/0.1039/0.6400/0.1788/0.7980] Loss: 0.4145\n",
      "Epoch: [32/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [82.0312/0.1212/0.7059/0.2069/0.8455] Loss: 0.4214\n",
      "Epoch: [32/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [81.8359/0.1182/0.7742/0.2051/0.8821] Loss: 0.4242\n",
      "Epoch: [32/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [87.0117/0.1379/0.7143/0.2312/0.9043] Loss: 0.4261\n",
      "Epoch: [32/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [83.9844/0.0552/0.4737/0.0989/0.8181] Loss: 0.4279\n",
      "Epoch: [32/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.8107/0.0951/0.5280/0.1612/0.8015] Loss: 425.9798\n",
      "Epoch: [33/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.2305/0.1218/0.8261/0.2123/0.9187] Loss: 0.3794\n",
      "Epoch: [33/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [85.1562/0.1173/0.6786/0.2000/0.8737] Loss: 0.3929\n",
      "Epoch: [33/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [84.9609/0.1006/0.5926/0.1720/0.8621] Loss: 0.3985\n",
      "Epoch: [33/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.6719/0.1475/0.6000/0.2368/0.8583] Loss: 0.4064\n",
      "Epoch: [33/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [85.8398/0.1169/0.6667/0.1989/0.8582] Loss: 0.4142\n",
      "Epoch: [33/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [88.0859/0.1944/0.8235/0.3146/0.9109] Loss: 0.4210\n",
      "Epoch: [33/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [90.8203/0.1683/0.6296/0.2656/0.8736] Loss: 0.4213\n",
      "Epoch: [33/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [84.2773/0.1053/0.6923/0.1827/0.8730] Loss: 0.4258\n",
      "Epoch: [33/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [85.8398/0.1210/0.7308/0.2077/0.8762] Loss: 0.4267\n",
      "Epoch: [33/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [83.2150/0.0888/0.5938/0.1545/0.8047] Loss: 425.5402\n",
      "Epoch: [34/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.1328/0.1829/0.7895/0.2970/0.9136] Loss: 0.3762\n",
      "Epoch: [34/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.5977/0.1348/0.7917/0.2303/0.9020] Loss: 0.3903\n",
      "Epoch: [34/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [82.8125/0.1152/0.7586/0.2000/0.8763] Loss: 0.3992\n",
      "Epoch: [34/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [88.5742/0.1475/0.5806/0.2353/0.8795] Loss: 0.4048\n",
      "Epoch: [34/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [87.5977/0.1915/0.6750/0.2983/0.8898] Loss: 0.4107\n",
      "Epoch: [34/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.8906/0.1630/0.6667/0.2619/0.8630] Loss: 0.4177\n",
      "Epoch: [34/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [84.6680/0.1453/0.7143/0.2415/0.8692] Loss: 0.4208\n",
      "Epoch: [34/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.3281/0.1250/0.7308/0.2135/0.9047] Loss: 0.4221\n",
      "Epoch: [34/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [87.3047/0.1029/0.6364/0.1772/0.8880] Loss: 0.4274\n",
      "Epoch: [34/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.3422/0.0964/0.5582/0.1644/0.8088] Loss: 424.0560\n",
      "Epoch: [35/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [86.1328/0.1382/0.6562/0.2283/0.8506] Loss: 0.3753\n",
      "Epoch: [35/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [89.8438/0.1132/0.5455/0.1875/0.8743] Loss: 0.3863\n",
      "Epoch: [35/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.0859/0.1181/0.6000/0.1974/0.8786] Loss: 0.3994\n",
      "Epoch: [35/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [84.3750/0.1098/0.7600/0.1919/0.9154] Loss: 0.4070\n",
      "Epoch: [35/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [88.2812/0.1221/0.7619/0.2105/0.8723] Loss: 0.4130\n",
      "Epoch: [35/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [80.0781/0.1101/0.9259/0.1969/0.9085] Loss: 0.4163\n",
      "Epoch: [35/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [89.2578/0.1638/0.5938/0.2568/0.8535] Loss: 0.4197\n",
      "Epoch: [35/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.6211/0.1208/0.7500/0.2081/0.9189] Loss: 0.4210\n",
      "Epoch: [35/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [85.2539/0.1394/0.7188/0.2335/0.8954] Loss: 0.4245\n",
      "Epoch: [35/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [84.1048/0.0914/0.5768/0.1579/0.8105] Loss: 423.5706\n",
      "Epoch: [36/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.5000/0.1250/0.9000/0.2195/0.9350] Loss: 0.3738\n",
      "Epoch: [36/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.7930/0.1159/0.8421/0.2038/0.9313] Loss: 0.3844\n",
      "Epoch: [36/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [90.1367/0.1333/0.5833/0.2171/0.9023] Loss: 0.3953\n",
      "Epoch: [36/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.5977/0.1469/0.8077/0.2485/0.9087] Loss: 0.4047\n",
      "Epoch: [36/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [83.9844/0.1124/0.7692/0.1961/0.8907] Loss: 0.4083\n",
      "Epoch: [36/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [85.0586/0.1090/0.5484/0.1818/0.8589] Loss: 0.4156\n",
      "Epoch: [36/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [86.5234/0.0780/0.5789/0.1375/0.8830] Loss: 0.4183\n",
      "Epoch: [36/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.1328/0.1074/0.6400/0.1839/0.8849] Loss: 0.4201\n",
      "Epoch: [36/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [89.1602/0.1053/0.5714/0.1778/0.8482] Loss: 0.4237\n",
      "Epoch: [36/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [85.7283/0.0964/0.5404/0.1636/0.8017] Loss: 425.8612\n",
      "Epoch: [37/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [87.5000/0.1135/0.8421/0.2000/0.9189] Loss: 0.3759\n",
      "Epoch: [37/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [87.1094/0.1348/0.6552/0.2235/0.8691] Loss: 0.3825\n",
      "Epoch: [37/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [88.7695/0.2273/0.6977/0.3429/0.8714] Loss: 0.3946\n",
      "Epoch: [37/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [86.5234/0.1168/0.4848/0.1882/0.8297] Loss: 0.4005\n",
      "Epoch: [37/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [89.1602/0.1271/0.6522/0.2128/0.8392] Loss: 0.4086\n",
      "Epoch: [37/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [87.4023/0.1733/0.8387/0.2873/0.9268] Loss: 0.4129\n",
      "Epoch: [37/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [87.2070/0.0930/0.4615/0.1548/0.8225] Loss: 0.4189\n",
      "Epoch: [37/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [84.6680/0.1503/0.7222/0.2488/0.8844] Loss: 0.4192\n",
      "Epoch: [37/100] Train Batch: [9000/9991] [A/P/R/F/ROC]: [85.1562/0.0994/0.6957/0.1739/0.8826] Loss: 0.4258\n",
      "Epoch: [37/100] Eval Batch: [92/92] [A/P/R/F/ROC]: [86.5722/0.1019/0.5371/0.1712/0.8072] Loss: 422.5033\n",
      "Epoch: [38/100] Train Batch: [1000/9991] [A/P/R/F/ROC]: [88.1836/0.1450/0.6786/0.2390/0.9082] Loss: 0.3727\n",
      "Epoch: [38/100] Train Batch: [2000/9991] [A/P/R/F/ROC]: [86.9141/0.1389/0.6667/0.2299/0.8729] Loss: 0.3832\n",
      "Epoch: [38/100] Train Batch: [3000/9991] [A/P/R/F/ROC]: [85.6445/0.1053/0.5926/0.1788/0.8263] Loss: 0.3926\n",
      "Epoch: [38/100] Train Batch: [4000/9991] [A/P/R/F/ROC]: [87.3047/0.1250/0.8182/0.2169/0.9340] Loss: 0.4016\n",
      "Epoch: [38/100] Train Batch: [5000/9991] [A/P/R/F/ROC]: [84.1797/0.1405/0.8966/0.2430/0.9268] Loss: 0.4069\n",
      "Epoch: [38/100] Train Batch: [6000/9991] [A/P/R/F/ROC]: [81.7383/0.1128/0.6111/0.1905/0.8088] Loss: 0.4134\n",
      "Epoch: [38/100] Train Batch: [7000/9991] [A/P/R/F/ROC]: [81.5430/0.0660/0.7222/0.1209/0.8017] Loss: 0.4141\n",
      "Epoch: [38/100] Train Batch: [8000/9991] [A/P/R/F/ROC]: [86.9141/0.1391/0.8400/0.2386/0.9402] Loss: 0.4197\n",
      "Epoch: [38/100] Train Batch: [8880/9991]"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Model Training\n",
    "# =============================================\n",
    "now = datetime.now().strftime('%m-%d_%H:%M')\n",
    "result_path = './results/{}_results.txt'.format(now)\n",
    "\n",
    "best_model = {}\n",
    "best = np.zeros(4)\n",
    "logger.info(\"\\nModel Training..\")\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "\n",
    "    if len(_train) % BATCH_SIZE == 0:\n",
    "        batch_num = int(len(_train)/BATCH_SIZE)\n",
    "    else:\n",
    "        batch_num = int(len(_train)/BATCH_SIZE) + 1\n",
    "\n",
    "    loss = .0\n",
    "    batches = make_batch(_train, BATCH_SIZE, word2id, book2genre)\n",
    "    step = 0\n",
    "\n",
    "    for batch in batches:\n",
    "        count = 0\n",
    "        correct = 0\n",
    "        positive_answer = 0\n",
    "        positive_actual = 0\n",
    "\n",
    "        sentences, adjacency_matrics, labels, genres = batch\n",
    "        input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "        input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "        input_labels = torch.tensor(labels, dtype=torch.long).cuda()\n",
    "        input_genres = torch.tensor(genres, dtype=torch.long).cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_sentences, input_adjacency_matrics, input_genres)\n",
    "        _loss = criterion(logits, input_labels).sum()\n",
    "        _loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += _loss.item()\n",
    "\n",
    "        step+=1\n",
    "\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r\" + \"Epoch: [{}/{}] Train Batch: [{}/{}]\".format(i+1, EPOCHS, step, batch_num))\n",
    "#         result.write(\"Epoch: [{}/{}] Train Batch: [{}/{}]\\n\".format(i+1, EPOCHS, step, batch_num))\n",
    "        if step % 1000 == 0:\n",
    "            long_logits = torch.argmax(logits, dim=1)\n",
    "            positive_answer += long_logits.sum().item()\n",
    "            positive_actual += (input_labels == 1.0).float().sum().item()\n",
    "            correct+=(long_logits*input_labels).sum().item()\n",
    "            count+= (long_logits==input_labels).sum().item()\n",
    "\n",
    "            accuracy = 100*float(count) / len(sentences)\n",
    "            if positive_answer == 0:\n",
    "                precision = 0.0\n",
    "            else:\n",
    "                precision = float(correct) / positive_answer\n",
    "            recall = float(correct) / positive_actual\n",
    "            if (precision + recall) == 0.0:\n",
    "                f1 = 0.0\n",
    "            else:\n",
    "                f1 = 2*precision*recall / (precision + recall)\n",
    "            auroc = roc_auc_score(input_labels.cpu().detach().numpy(), logits[:, 1].cpu().detach().numpy())\n",
    "\n",
    "            print(\" [A/P/R/F/ROC]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}/{:.4f}] Loss: {:.4f}\".format(accuracy, precision, recall, f1, auroc, loss/1000))\n",
    "            result = open(result_path, 'a')\n",
    "            result.write(\"Epoch: [{}/{}] Train Batch: [{}/{}] [A/P/R/F/ROC]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}/{:.4f}] Loss: {:.4f}\\n\".format(i+1, EPOCHS, step, batch_num, accuracy, precision, recall, f1, auroc, loss/1000))\n",
    "            result.close()\n",
    "            loss = .0\n",
    "\n",
    "    if (i+1) % 1 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        batches = make_batch(_validation, BATCH_SIZE, word2id, book2genre, False)\n",
    "        if len(_validation) % BATCH_SIZE == 0:\n",
    "            batch_num = int(len(_validation)/BATCH_SIZE)\n",
    "        else:\n",
    "            batch_num = int(len(_validation)/BATCH_SIZE) + 1\n",
    "\n",
    "        step = 0\n",
    "        count = 0\n",
    "        correct = 0\n",
    "        positive_answer = 0\n",
    "        positive_actual = 0\n",
    "        for batch in batches:\n",
    "            sentences, adjacency_matrics, labels, genres = batch\n",
    "            input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "            input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "            input_labels = torch.tensor(labels, dtype=torch.long).cuda()\n",
    "            input_genres = torch.tensor(genres, dtype=torch.long).cuda()\n",
    "            logits = model(input_sentences, input_adjacency_matrics, input_genres)\n",
    "            long_logits = torch.argmax(logits, dim=1)\n",
    "\n",
    "            if step == 0:\n",
    "                entire_labels = input_labels.cpu().detach()\n",
    "                entire_logits = logits.cpu().detach()\n",
    "            else:\n",
    "                entire_labels = torch.cat([entire_labels, input_labels.cpu().detach()], dim=0)\n",
    "                entire_logits = torch.cat([entire_logits, logits.cpu().detach()], dim=0)\n",
    "            positive_answer += long_logits.sum().item()\n",
    "            positive_actual += (input_labels == 1.0).float().sum().item()\n",
    "            correct+=(long_logits*input_labels).sum().item()\n",
    "            count+= (long_logits==input_labels).sum().item()\n",
    "            step+=1\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            sys.stdout.write(\"\\r\" + \"Epoch: [{}/{}] Eval Batch: [{}/{}]\".format(i+1, EPOCHS, step, batch_num))\n",
    "#             result.write(\"Epoch: [{}/{}] Eval Batch: [{}/{}]\\n\".format(i+1, EPOCHS, step, batch_num))\n",
    "\n",
    "        accuracy = 100*float(count)/len(_validation)\n",
    "        if positive_answer == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = float(correct)/positive_answer\n",
    "        recall = float(correct)/positive_actual\n",
    "        if (precision+recall) == 0.0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "        auroc = roc_auc_score(entire_labels.numpy(), entire_logits[:, 1].numpy())\n",
    "        del entire_labels\n",
    "        del entire_logits\n",
    "        print(\" [A/P/R/F/ROC]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}/{:.4f}] Loss: {:.4f}\".format(accuracy, precision, recall, f1, auroc, loss))\n",
    "        result = open(result_path, 'a')\n",
    "        result.write(\"Epoch: [{}/{}] Eval Batch: [{}/{}] [A/P/R/F/ROC]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}/{:.4f}] Loss: {:.4f}\\n\".format(i+1, EPOCHS, step, batch_num, accuracy, precision, recall, f1, auroc, loss))\n",
    "        result.close()\n",
    "        \n",
    "        if auroc > best[3]:\n",
    "            past = best[3]\n",
    "            best = precision, recall, f1, auroc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            if past != 0:\n",
    "                print(\"Current Best:)\")\n",
    "                result = open(result_path, 'a')\n",
    "                result.write(\"Current Best:)\\n\")\n",
    "                result.close()\n",
    "                modelname = '{}_only_attention{}'.format(now, i+1)\n",
    "                torch.save(best_model, './models/' + modelname + '.pt')\n",
    "                \n",
    "                ## Start Testing\n",
    "                batches = make_batch(_test, BATCH_SIZE, word2id, book2genre, False)\n",
    "                if len(_test) % BATCH_SIZE == 0:\n",
    "                    batch_num = int(len(_test)/BATCH_SIZE)\n",
    "                else:\n",
    "                    batch_num = int(len(_test)/BATCH_SIZE) + 1\n",
    "\n",
    "                step = 0\n",
    "                count = 0\n",
    "                correct = 0\n",
    "                positive_answer = 0\n",
    "                positive_actual = 0\n",
    "                for batch in batches:\n",
    "                    sentences, adjacency_matrics, labels, genres = batch\n",
    "                    input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "                    input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "                    input_labels = torch.tensor(labels, dtype=torch.long).cuda()\n",
    "                    input_genres = torch.tensor(genres, dtype=torch.long).cuda()\n",
    "                    logits = model(input_sentences, input_adjacency_matrics, input_genres)\n",
    "                    long_logits = torch.argmax(logits, dim=1)\n",
    "\n",
    "                    if step == 0:\n",
    "                        entire_labels = input_labels.cpu().detach()\n",
    "                        entire_logits = logits.cpu().detach()\n",
    "                    else:\n",
    "                        entire_labels = torch.cat([entire_labels, input_labels.cpu().detach()], dim=0)\n",
    "                        entire_logits = torch.cat([entire_logits, logits.cpu().detach()], dim=0)\n",
    "                    positive_answer += long_logits.sum().item()\n",
    "                    positive_actual += (input_labels == 1.0).float().sum().item()\n",
    "                    correct+=(long_logits*input_labels).sum().item()\n",
    "                    count+= (long_logits==input_labels).sum().item()\n",
    "                    step+=1\n",
    "\n",
    "                    sys.stdout.flush()\n",
    "                    sys.stdout.write(\"\\r\" + \"Test Batch: [{}/{}]\".format(step, batch_num))\n",
    "\n",
    "                accuracy = 100*float(count)/len(_test)\n",
    "                if positive_answer == 0:\n",
    "                    precision = 0.0\n",
    "                else:\n",
    "                    precision = float(correct)/positive_answer\n",
    "                recall = float(correct)/positive_actual\n",
    "                if (precision+recall) == 0.0:\n",
    "                    f1 = 0.0\n",
    "                else:\n",
    "                    f1 = 2*precision*recall/(precision+recall)\n",
    "                auroc = roc_auc_score(entire_labels.numpy(), entire_logits[:, 1].numpy())\n",
    "\n",
    "                print(\"{} [A/P/R/F/ROC]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}/{:.4f}]\".format(modelname, accuracy, precision, recall, f1, auroc))\n",
    "                result = open(result_path, 'a')\n",
    "                result.write(\"{} [A/P/R/F/ROC]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}/{:.4f}]\\n\".format(modelname, accuracy, precision, recall, f1, auroc))\n",
    "                result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
