{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ProcessPoolExecutor as prpExecutor\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Data File Path\n",
    "# ============================\n",
    "TRAIN_DATA_FILE_PATH = \"/hdd1/Spoiler_Detection/ACL/INGGEOL/train_10000.json\"\n",
    "VALIDATION_DATA_FILE_PATH = \"/hdd1/Spoiler_Detection/ACL/INGGEOL/validation_10000.json\"\n",
    "PRE_TRAINED_WORD_EMBEDDING_FILE_PATH = \"../word_embedding_10000.npy\"\n",
    "\n",
    "# ============================\n",
    "# Model Hyper Parameter\n",
    "# ============================\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_STATES = [50, 50]\n",
    "\n",
    "# ============================\n",
    "# Training Hyper Parameter\n",
    "# ============================\n",
    "EPOCHS = 300\n",
    "LEARNING_RATE = 0.00005\n",
    "BATCH_SIZE = 256\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DROPOUT_RATE = 0.5\n",
    "RANDOM_SEED = 26\n",
    "\n",
    "# ============================\n",
    "# Set Random Seed\n",
    "# ============================\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Data Pre-Processing\n",
    "# ============================\n",
    "def load_data(train_file_path, validation_file_path):\n",
    "    with open(train_file_path) as f:\n",
    "        train = json.load(f)[\"data\"]\n",
    "    with open(validation_file_path) as f:\n",
    "        validation = json.load(f)[\"data\"]\n",
    "        \n",
    "    return train, validation\n",
    "    \n",
    "def make_dictionary(train, validation):\n",
    "    data = []\n",
    "    data += train\n",
    "    data += validation\n",
    "    \n",
    "    maximum_length = max([len(line.split(\"\\t\")[0].split()) for line in data])\n",
    "\n",
    "    word2id = {\"<PAD>\":0}\n",
    "    id2word = [\"<PAD>\"]\n",
    "    edge2id = {}\n",
    "    id2edge = []\n",
    "    \n",
    "    for line in data:\n",
    "        tokens = line.split(\"\\t\")\n",
    "        for word in tokens[0].split():\n",
    "            if word not in word2id:\n",
    "                word2id[word] = len(word2id)\n",
    "                id2word.append(word)\n",
    "        for edges in tokens[2:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            if edge not in edge2id:\n",
    "                edge2id[edge] = len(edge2id)\n",
    "                id2edge.append(edge)\n",
    "\n",
    "    return word2id, id2word, edge2id, id2edge, maximum_length\n",
    "\n",
    "def make_input_data_as_index(_train, _validation, word2id, edge2id):\n",
    "    train, validation = [], []\n",
    "    for line in _train:\n",
    "        tokens = line.split(\"\\t\")\n",
    "        tokens[0] = [word2id[word] for word in tokens[0].split()]\n",
    "        _edges = []\n",
    "        for edges in tokens[2:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            _edges.append([start, end, edge2id[edge]])\n",
    "        train.append([tokens[0], tokens[1], _edges])\n",
    "\n",
    "    for line in _validation:\n",
    "        tokens = line.split(\"\\t\")\n",
    "        tokens[0] = [word2id[word] for word in tokens[0].split()]\n",
    "        _edges = []\n",
    "        for edges in tokens[2:]:\n",
    "            _tokens = edges.split(\":\")\n",
    "            if len(_tokens) != 3:\n",
    "                start, end = _tokens[0], _tokens[1]\n",
    "                edge = \":\".join(_tokens[2:])\n",
    "            else:\n",
    "                start, end, edge = _tokens\n",
    "            _edges.append([start, end, edge2id[edge]])\n",
    "        validation.append([tokens[0], tokens[1], _edges])\n",
    "\n",
    "    return train, validation\n",
    "\n",
    "def make_input_adjacency_matrix(line):\n",
    "    words, label, edges = line[0], float(line[1]), line[2]\n",
    "    adjacency_matrix = make_adjacency_matrix(np.asarray(edges), len(words))\n",
    "    \n",
    "    return [words, adjacency_matrix, label]\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    rowsum = np.asarray(matrix.sum(1))\n",
    "    row_inv = np.power(np.sqrt(rowsum), -1).flatten()\n",
    "    row_inv[np.isinf(row_inv)] = 0.\n",
    "    row_matrix_inv = sp.diags(row_inv)\n",
    "    matrix = row_matrix_inv.dot(matrix)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def sparse_matrix_to_torch_sparse_tensor(sparse_matrix, maximum_length):\n",
    "    sparse_matrix = sparse_matrix.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_matrix.row, sparse_matrix.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_matrix.data)\n",
    "    shape = torch.Size((maximum_length, maximum_length))\n",
    "\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def make_adjacency_matrix(edges, num_words):\n",
    "    adjacency_matrix = sp.coo_matrix(\n",
    "        (np.ones(len(edges)), (edges[:, 0].astype(np.int32), edges[:, 1].astype(np.int32))),\n",
    "        shape=(num_words, num_words),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    identity_matrix = sp.coo_matrix(\n",
    "        (np.ones(len(edges)), (np.arange(len(edges)), np.arange(len(edges)))),\n",
    "        shape=(num_words, num_words),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    adjacency_matrix = adjacency_matrix + identity_matrix + adjacency_matrix.transpose()\n",
    "    normalized_adjacency_matrix = normalize_matrix(adjacency_matrix)\n",
    "    \n",
    "    return normalized_adjacency_matrix\n",
    "\n",
    "def load_pre_trained_word_embedding(word_embedding_file_path):\n",
    "    return torch.from_numpy(np.load(word_embedding_file_path).astype(np.float32))\n",
    "\n",
    "def make_batch(data, batch_size, is_train=True):\n",
    "    indices = np.arange(len(data))\n",
    "    if is_train:\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    if len(data) % batch_size == 0:\n",
    "        batch_num = int(len(data)/batch_size)\n",
    "    else:\n",
    "        batch_num = int(len(data)/batch_size) + 1\n",
    "        \n",
    "    for i in range(batch_num):\n",
    "        left = i*batch_size\n",
    "        right = min((i+1)*batch_size, len(data))\n",
    "        \n",
    "        sentences = []\n",
    "        adjacency_matrics = []\n",
    "        labels = []\n",
    "        \n",
    "        for j in indices[left:right]:\n",
    "            sentences.append(data[j][0])\n",
    "            adjacency_matrics.append(data[j][1])\n",
    "            labels.append(data[j][2])\n",
    "        \n",
    "        yield sentences, adjacency_matrics, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data...\n",
      "Make Dictionary...\n",
      "Make Input as Index...\n",
      "Make Adjacency Matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n",
      "/home/buru/python3_venv/lib/python3.5/site-packages/ipykernel_launcher.py:83: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "Make Sparse Tensor...\n",
      "35\n",
      "Load Pre-trained Word Embedding...\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Data Pre Processing\n",
    "# ============================\n",
    "print(\"Load Data...\")\n",
    "train, validation = load_data(TRAIN_DATA_FILE_PATH, VALIDATION_DATA_FILE_PATH)\n",
    "\n",
    "print(\"Make Dictionary...\")\n",
    "word2id, id2word, edge2id, id2edge, maximum_length = make_dictionary(train, validation)\n",
    "\n",
    "print(\"Make Input as Index...\")\n",
    "train, validation = make_input_data_as_index(train, validation, word2id, edge2id)\n",
    "\n",
    "print(\"Make Adjacency Matrix...\")\n",
    "start = time.time()\n",
    "pool = prpExecutor(max_workers=16)\n",
    "train = list(pool.map(make_input_adjacency_matrix, train))\n",
    "validation = list(pool.map(make_input_adjacency_matrix, validation))\n",
    "del pool\n",
    "print(int(time.time() - start))\n",
    "\n",
    "print(\"Make Sparse Tensor...\")\n",
    "start = time.time()\n",
    "for line in train+validation:\n",
    "    line[0] += [0] * (maximum_length - len(line[0]))\n",
    "    line[1] = sparse_matrix_to_torch_sparse_tensor(line[1], maximum_length)\n",
    "print(int(time.time() - start))\n",
    "\n",
    "print(\"Load Pre-trained Word Embedding...\")\n",
    "word_embedding = load_pre_trained_word_embedding(PRE_TRAINED_WORD_EMBEDDING_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Pre-trained Word Embedding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Pre-trained Word Embedding...\")\n",
    "word_embedding = load_pre_trained_word_embedding(PRE_TRAINED_WORD_EMBEDDING_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Model\n",
    "# ============================\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GCNLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(self.input_dim, self.output_dim))\n",
    "        nn.init.xavier_normal_(self.weight)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(self.output_dim))\n",
    "\n",
    "    def forward(self, x, adj_matrics):\n",
    "        x = torch.matmul(adj_matrics, x)\n",
    "        output = torch.matmul(x, self.weight)\n",
    "        output = output + self.bias\n",
    "\n",
    "        return output\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_words, embedding_dim, hidden_dim, maximum_length, pre_trained, dropout_rate):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.num_words = num_words\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.maximum_length = maximum_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # =============================================\n",
    "        # Data Preparation\n",
    "        # =============================================\t\t\n",
    "        self.word_embedding \\\n",
    "        = nn.Embedding.from_pretrained(pre_trained, padding_idx = 0)\n",
    "#         self.word_embedding.from_pretrained(pre_trained)\n",
    "        \n",
    "        self.gcn_layer_1 \\\n",
    "        = GCNLayer(self.embedding_dim, self.hidden_dim[0])\n",
    "        self.gcn_layer_2 \\\n",
    "        = GCNLayer(self.hidden_dim[0], self.hidden_dim[1])\n",
    "        \n",
    "        self.max_pooling = nn.MaxPool1d(self.maximum_length)\n",
    "        self.output_layer = nn.Linear(self.hidden_dim[1], 1)\n",
    "\n",
    "    def forward(self, sentences, adjacency_matrics, batch_size):\n",
    "        embedded_words = self.word_embedding(sentences)\n",
    "        gcn_1 = self.gcn_layer_1(embedded_words, adjacency_matrics)\n",
    "        gcn_1 = F.relu(gcn_1)\n",
    "        gcn_1 = F.dropout(gcn_1, self.dropout_rate)\n",
    "        gcn_2 = self.gcn_layer_2(gcn_1, adjacency_matrics)\n",
    "        gcn_2 = F.relu(gcn_2)\n",
    "        sentence_representations = self.max_pooling(gcn_2.transpose(1, 2)).squeeze()\n",
    "        sentence_representations = F.dropout(sentence_representations, self.dropout_rate)\n",
    "        output = self.output_layer(sentence_representations)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initializing..\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Model Initialize\n",
    "# =============================================\n",
    "print(\"Model Initializing..\")\n",
    "pos_weight = 20*torch.ones([1]).cuda()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "model = Model(len(word2id), EMBEDDING_DIM, HIDDEN_STATES, maximum_length, word_embedding, DROPOUT_RATE).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Training..\n",
      "\n",
      "Epoch: [1/300] Batch: [463/463] Loss: 475.4751196503639 [A/P/R/F]: [96.5564/0.0444/0.0182/0.0258]\n",
      " Current Best:)\n",
      "Epoch: [2/300] Batch: [463/463] Loss: 457.76973366737366 [A/P/R/F]: [97.2026/0.0000/0.0000/0.0000]\n",
      "Epoch: [3/300] Batch: [463/463] Loss: 445.254254758358 [A/P/R/F]: [97.2178/0.0909/0.0121/0.0214]\n",
      "Epoch: [4/300] Batch: [463/463] Loss: 437.50181555747986 [A/P/R/F]: [96.9745/0.0526/0.0121/0.0197]\n",
      "Epoch: [5/300] Batch: [463/463] Loss: 430.921693444252 [A/P/R/F]: [96.7769/0.0392/0.0121/0.0185]\n",
      "Epoch: [6/300] Batch: [463/463] Loss: 426.68244674801826 [A/P/R/F]: [96.4652/0.0588/0.0273/0.0373]\n",
      " Current Best:)\n",
      "Epoch: [7/300] Batch: [463/463] Loss: 423.02322110533714 [A/P/R/F]: [96.0699/0.0807/0.0545/0.0651]\n",
      " Current Best:)\n",
      "Epoch: [8/300] Batch: [463/463] Loss: 418.24900379776955 [A/P/R/F]: [95.8039/0.0858/0.0697/0.0769]\n",
      " Current Best:)\n",
      "Epoch: [9/300] Batch: [463/463] Loss: 417.98925602436066 [A/P/R/F]: [95.3630/0.0808/0.0818/0.0813]\n",
      " Current Best:)\n",
      "Epoch: [10/300] Batch: [463/463] Loss: 413.6729918420315 [A/P/R/F]: [95.0893/0.0885/0.1030/0.0952]\n",
      " Current Best:)\n",
      "Epoch: [11/300] Batch: [463/463] Loss: 412.1173006296158 [A/P/R/F]: [95.5150/0.0912/0.0879/0.0895]\n",
      "Epoch: [12/300] Batch: [463/463] Loss: 409.8876853585243 [A/P/R/F]: [95.1501/0.0769/0.0848/0.0807]\n",
      "Epoch: [13/300] Batch: [463/463] Loss: 407.3657235503197 [A/P/R/F]: [94.7624/0.0757/0.0970/0.0850]\n",
      "Epoch: [14/300] Batch: [463/463] Loss: 406.7624853551388 [A/P/R/F]: [94.9069/0.0894/0.1121/0.0995]\n",
      " Current Best:)\n",
      "Epoch: [15/300] Batch: [463/463] Loss: 405.65529400110245 [A/P/R/F]: [94.8765/0.0885/0.1121/0.0989]\n",
      "Epoch: [16/300] Batch: [463/463] Loss: 404.60468742251396 [A/P/R/F]: [94.6940/0.0911/0.1242/0.1051]\n",
      " Current Best:)\n",
      "Epoch: [17/300] Batch: [463/463] Loss: 403.1639446616173 [A/P/R/F]: [94.6712/0.0994/0.1394/0.1160]\n",
      " Current Best:)\n",
      "Epoch: [18/300] Batch: [463/463] Loss: 402.2676774263382 [A/P/R/F]: [94.6104/0.0959/0.1364/0.1126]\n",
      "Epoch: [19/300] Batch: [463/463] Loss: 399.7592141032219 [A/P/R/F]: [94.9145/0.0955/0.1212/0.1068]\n",
      "Epoch: [20/300] Batch: [463/463] Loss: 398.97197261452675 [A/P/R/F]: [94.2683/0.0954/0.1515/0.1171]\n",
      " Current Best:)\n",
      "Epoch: [21/300] Batch: [463/463] Loss: 399.27703949809074 [A/P/R/F]: [94.1163/0.0934/0.1545/0.1164]\n",
      "Epoch: [22/300] Batch: [463/463] Loss: 396.5958546102047 [A/P/R/F]: [94.5648/0.0896/0.1273/0.1051]\n",
      "Epoch: [23/300] Batch: [463/463] Loss: 395.2592162191868 [A/P/R/F]: [93.9111/0.0933/0.1636/0.1188]\n",
      " Current Best:)\n",
      "Epoch: [24/300] Batch: [463/463] Loss: 395.5087553858757 [A/P/R/F]: [94.0479/0.0844/0.1394/0.1051]\n",
      "Epoch: [25/300] Batch: [463/463] Loss: 392.93496385216713 [A/P/R/F]: [94.3748/0.1042/0.1636/0.1274]\n",
      " Current Best:)\n",
      "Epoch: [26/300] Batch: [463/463] Loss: 391.9074865579605 [A/P/R/F]: [93.8350/0.0958/0.1727/0.1232]\n",
      "Epoch: [27/300] Batch: [463/463] Loss: 393.0298438966274 [A/P/R/F]: [93.9795/0.0962/0.1667/0.1220]\n",
      "Epoch: [28/300] Batch: [463/463] Loss: 392.3629118204117 [A/P/R/F]: [94.3139/0.0996/0.1576/0.1221]\n",
      "Epoch: [29/300] Batch: [463/463] Loss: 389.428435087204 [A/P/R/F]: [94.0555/0.1007/0.1727/0.1272]\n",
      "Epoch: [30/300] Batch: [463/463] Loss: 390.27671480178833 [A/P/R/F]: [93.6526/0.0921/0.1727/0.1201]\n",
      "Epoch: [31/300] Batch: [463/463] Loss: 389.21993786096573 [A/P/R/F]: [94.1771/0.1121/0.1909/0.1413]\n",
      " Current Best:)\n",
      "Epoch: [32/300] Batch: [463/463] Loss: 387.41208589076996 [A/P/R/F]: [93.4398/0.0968/0.1939/0.1292]\n",
      "Epoch: [33/300] Batch: [463/463] Loss: 383.63050255179405 [A/P/R/F]: [93.1965/0.0876/0.1818/0.1182]\n",
      "Epoch: [34/300] Batch: [463/463] Loss: 386.2847611606121 [A/P/R/F]: [93.4246/0.0953/0.1909/0.1271]\n",
      "Epoch: [35/300] Batch: [463/463] Loss: 385.19993618130684 [A/P/R/F]: [93.5538/0.1111/0.2242/0.1486]\n",
      " Current Best:)\n",
      "Epoch: [36/300] Batch: [463/463] Loss: 385.40132105350494 [A/P/R/F]: [94.0935/0.1153/0.2030/0.1471]\n",
      "Epoch: [37/300] Batch: [463/463] Loss: 384.5200534462929 [A/P/R/F]: [93.5006/0.0866/0.1667/0.1140]\n",
      "Epoch: [38/300] Batch: [463/463] Loss: 382.70708683133125 [A/P/R/F]: [92.7784/0.0984/0.2303/0.1379]\n",
      "Epoch: [39/300] Batch: [463/463] Loss: 383.32462725043297 [A/P/R/F]: [93.9491/0.1180/0.2182/0.1532]\n",
      " Current Best:)\n",
      "Epoch: [40/300] Batch: [463/463] Loss: 382.23384016752243 [A/P/R/F]: [93.4778/0.0951/0.1879/0.1263]\n",
      "Epoch: [41/300] Batch: [463/463] Loss: 382.2341079413891 [A/P/R/F]: [93.7362/0.1067/0.2030/0.1399]\n",
      "Epoch: [42/300] Batch: [463/463] Loss: 380.2757006883621 [A/P/R/F]: [93.4322/0.0880/0.1727/0.1166]\n",
      "Epoch: [43/300] Batch: [463/463] Loss: 380.315139234066 [A/P/R/F]: [92.9076/0.0996/0.2273/0.1385]\n",
      "Epoch: [44/300] Batch: [463/463] Loss: 381.49247244000435 [A/P/R/F]: [93.6450/0.1131/0.2242/0.1504]\n",
      "Epoch: [45/300] Batch: [463/463] Loss: 379.02852818369865 [A/P/R/F]: [93.0901/0.0940/0.2030/0.1285]\n",
      "Epoch: [46/300] Batch: [463/463] Loss: 378.60410514473915 [A/P/R/F]: [93.5006/0.0930/0.1818/0.1231]\n",
      "Epoch: [47/300] Batch: [463/463] Loss: 378.6649021804333 [A/P/R/F]: [94.0327/0.1151/0.2061/0.1477]\n",
      "Epoch: [48/300] Batch: [463/463] Loss: 377.6820683181286 [A/P/R/F]: [93.5386/0.1108/0.2242/0.1483]\n",
      "Epoch: [49/300] Batch: [463/463] Loss: 375.1181257367134 [A/P/R/F]: [93.0293/0.0907/0.1970/0.1242]\n",
      "Epoch: [50/300] Batch: [463/463] Loss: 375.36977341771126 [A/P/R/F]: [93.0901/0.1061/0.2364/0.1465]\n",
      "Epoch: [51/300] Batch: [463/463] Loss: 375.52001467347145 [A/P/R/F]: [93.3333/0.1121/0.2394/0.1527]\n",
      "Epoch: [52/300] Batch: [463/463] Loss: 376.14338701963425 [A/P/R/F]: [93.7818/0.1127/0.2152/0.1479]\n",
      "Epoch: [53/300] Batch: [463/463] Loss: 373.97355872392654 [A/P/R/F]: [93.3257/0.1017/0.2121/0.1375]\n",
      "Epoch: [54/300] Batch: [463/463] Loss: 372.7689484655857 [A/P/R/F]: [93.0369/0.1104/0.2515/0.1534]\n",
      " Current Best:)\n",
      "Epoch: [55/300] Batch: [463/463] Loss: 372.73170471191406 [A/P/R/F]: [93.1965/0.1093/0.2394/0.1500]\n",
      "Epoch: [56/300] Batch: [463/463] Loss: 372.85080909729004 [A/P/R/F]: [93.0217/0.0917/0.2000/0.1257]\n",
      "Epoch: [57/300] Batch: [463/463] Loss: 370.47495168447495 [A/P/R/F]: [93.2041/0.1017/0.2182/0.1387]\n",
      "Epoch: [58/300] Batch: [463/463] Loss: 369.9224487543106 [A/P/R/F]: [93.5918/0.1096/0.2182/0.1459]\n",
      "Epoch: [59/300] Batch: [463/463] Loss: 368.22233191132545 [A/P/R/F]: [93.0445/0.0921/0.2000/0.1261]\n",
      "Epoch: [60/300] Batch: [463/463] Loss: 370.0701680779457 [A/P/R/F]: [92.7404/0.0936/0.2182/0.1310]\n",
      "Epoch: [61/300] Batch: [463/463] Loss: 371.4528952538967 [A/P/R/F]: [92.5124/0.1040/0.2606/0.1487]\n",
      "Epoch: [62/300] Batch: [463/463] Loss: 367.95036417245865 [A/P/R/F]: [93.2801/0.1131/0.2455/0.1549]\n",
      " Current Best:)\n",
      "Epoch: [63/300] Batch: [463/463] Loss: 368.1276107132435 [A/P/R/F]: [93.5538/0.1134/0.2303/0.1520]\n",
      "Epoch: [64/300] Batch: [463/463] Loss: 367.7817956805229 [A/P/R/F]: [93.4017/0.1021/0.2091/0.1372]\n",
      "Epoch: [65/300] Batch: [463/463] Loss: 366.94919392466545 [A/P/R/F]: [92.6796/0.0958/0.2273/0.1348]\n",
      "Epoch: [66/300] Batch: [463/463] Loss: 367.2100149989128 [A/P/R/F]: [93.1433/0.1006/0.2182/0.1377]\n",
      "Epoch: [67/300] Batch: [463/463] Loss: 365.5555963218212 [A/P/R/F]: [92.7404/0.1039/0.2485/0.1466]\n",
      "Epoch: [68/300] Batch: [463/463] Loss: 364.234836012125 [A/P/R/F]: [92.4439/0.1066/0.2727/0.1533]\n",
      "Epoch: [69/300] Batch: [463/463] Loss: 366.17796498537064 [A/P/R/F]: [92.6188/0.0938/0.2242/0.1323]\n",
      "Epoch: [70/300] Batch: [463/463] Loss: 363.89200058579445 [A/P/R/F]: [93.0673/0.1141/0.2606/0.1587]\n",
      " Current Best:)\n",
      "Epoch: [71/300] Batch: [58/463]"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Model Training\n",
    "# =============================================\n",
    "best_model = {}\n",
    "best = np.zeros(3)\n",
    "print(\"Model Training..\\n\")\n",
    "for i in range(EPOCHS):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    \n",
    "    if len(train) % BATCH_SIZE == 0:\n",
    "        batch_num = int(len(train)/BATCH_SIZE)\n",
    "    else:\n",
    "        batch_num = int(len(train)/BATCH_SIZE) + 1\n",
    "    \n",
    "    loss = .0\n",
    "    batches = make_batch(train, BATCH_SIZE)\n",
    "    step = 0\n",
    "\n",
    "    for batch in batches:\n",
    "        sentences, adjacency_matrics, labels = batch\n",
    "        input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "        input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "        input_labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(dim=1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_sentences, input_adjacency_matrics, len(sentences))\n",
    "        _loss = criterion(logits, input_labels).sum()\n",
    "        _loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += _loss.item()\n",
    "        step+=1\n",
    "        \n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r\" + \"Epoch: [{}/{}] Batch: [{}/{}]\".format(i+1, EPOCHS, step, batch_num))\n",
    "    \n",
    "    if (i+1) % 1 == 0:\n",
    "        model.eval()\n",
    "        \n",
    "        batches = make_batch(validation, BATCH_SIZE)\n",
    "        if len(validation) % BATCH_SIZE == 0:\n",
    "            batch_num = int(len(validation)/BATCH_SIZE)\n",
    "        else:\n",
    "            batch_num = int(len(validation)/BATCH_SIZE) + 1\n",
    "        \n",
    "        step = 0\n",
    "        count = 0\n",
    "        correct = 0\n",
    "        positive_answer = 0\n",
    "        positive_actual = 0\n",
    "        for batch in batches:\n",
    "            sentences, adjacency_matrics, labels = batch\n",
    "            input_sentences = torch.tensor(sentences, dtype = torch.long).cuda()\n",
    "            input_adjacency_matrics = torch.stack([matrix.to_dense() for matrix in adjacency_matrics], dim=0).cuda()\n",
    "            input_labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(dim=1).cuda()\n",
    "            logits = model(input_sentences, input_adjacency_matrics, len(sentences))\n",
    "\n",
    "            predicted = (logits > 0.5).float()\n",
    "            positive_answer += predicted.sum().item()\n",
    "            positive_actual += (input_labels == 1.0).float().sum().item()\n",
    "            correct+=(predicted*input_labels).sum().item()\n",
    "            count+= (predicted==input_labels).sum().item()\n",
    "            step+=1\n",
    "\n",
    "        accuracy = 100*float(count)/len(validation)\n",
    "        if positive_answer == 0:\n",
    "            precision = 0.0\n",
    "        else:\n",
    "            precision = float(correct)/positive_answer\n",
    "        recall = float(correct)/positive_actual\n",
    "        if (precision+recall) == 0.0:\n",
    "            f1 = 0.0\n",
    "        else:\n",
    "            f1 = 2*precision*recall/(precision+recall)\n",
    "        print(\" Loss: {} [A/P/R/F]: [{:.4f}/{:.4f}/{:.4f}/{:.4f}]\".format(loss, accuracy, precision, recall, f1))\n",
    "        if f1 > best[2]:\n",
    "            best = precision, recall, f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print(\" Current Best:)\")\n",
    "    \n",
    "    else:\n",
    "        print(\" Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for line in train:\n",
    "    if line[2] == 1.0:\n",
    "        count+=1\n",
    "print(100*float(count)/len(train))\n",
    "\n",
    "count = 0\n",
    "for line in validation:\n",
    "    if line[2] == 1.0:\n",
    "        count+=1\n",
    "print(100*float(count)/len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model, \"../best_model_without_fine_tune.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(49842, 50, padding_idx=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.1910e-01, 5.2540e-04, 3.5943e-01, 5.2106e-01, 9.2570e-01, 1.1660e-01,\n",
       "         7.9218e-01, 2.4372e-01, 6.2472e-01, 4.5988e-01, 3.2842e-01, 5.4276e-01,\n",
       "         1.8415e-01, 2.4003e-01, 2.2915e-01, 1.8152e-01, 7.1876e-01, 4.1605e-01,\n",
       "         1.3763e-02, 9.5724e-02, 2.7980e-01, 6.1360e-01, 9.2846e-02, 6.9006e-01,\n",
       "         2.9711e-01, 5.1656e-01, 9.7445e-01, 8.9844e-02, 1.9295e-01, 6.4581e-01,\n",
       "         5.9681e-01, 1.5447e-02, 4.5644e-02, 6.5717e-01, 6.6877e-01, 5.5065e-01,\n",
       "         7.0487e-01, 9.9404e-01, 3.5898e-01, 3.2252e-01, 4.0845e-02, 7.5114e-03,\n",
       "         5.9773e-01, 6.4211e-02, 8.9571e-02, 5.2875e-01, 3.2387e-01, 9.6272e-01,\n",
       "         4.9603e-01, 6.7986e-01]),\n",
       " tensor([5.1910e-01, 5.2540e-04, 3.5943e-01, 5.2106e-01, 9.2570e-01, 1.1660e-01,\n",
       "         7.9218e-01, 2.4372e-01, 6.2472e-01, 4.5988e-01, 3.2842e-01, 5.4276e-01,\n",
       "         1.8415e-01, 2.4003e-01, 2.2915e-01, 1.8152e-01, 7.1876e-01, 4.1605e-01,\n",
       "         1.3763e-02, 9.5724e-02, 2.7980e-01, 6.1360e-01, 9.2846e-02, 6.9006e-01,\n",
       "         2.9711e-01, 5.1656e-01, 9.7445e-01, 8.9844e-02, 1.9295e-01, 6.4581e-01,\n",
       "         5.9681e-01, 1.5447e-02, 4.5644e-02, 6.5717e-01, 6.6877e-01, 5.5065e-01,\n",
       "         7.0487e-01, 9.9404e-01, 3.5898e-01, 3.2252e-01, 4.0845e-02, 7.5114e-03,\n",
       "         5.9773e-01, 6.4211e-02, 8.9571e-02, 5.2875e-01, 3.2387e-01, 9.6272e-01,\n",
       "         4.9603e-01, 6.7986e-01], device='cuda:0'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding[1,:50], model.word_embedding.weight[1,: 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([49842, 300]), Embedding(49842, 300, padding_idx=0))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding.shape, model.word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
